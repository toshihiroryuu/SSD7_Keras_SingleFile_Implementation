{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ssd7testathul.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toshihiroryuu/SSD7_Keras_SingleFile_Implementation/blob/master/ssd7testathul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "G76VvBFpMAoV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XxxL7ItQNFSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, ELU, Reshape, Concatenate, Activation\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.engine.topology import InputSpec\n",
        "from keras.engine.topology import Layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7CI75GNNTuY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import warnings\n",
        "import cv2\n",
        "import random\n",
        "import sklearn.utils\n",
        "from copy import deepcopy\n",
        "from PIL import Image\n",
        "import csv\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWOg9N2qNc8f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ssd_box_encode_decode_utils\n",
        "\n",
        "def iou(boxes1, boxes2, coords='centroids'):\n",
        "   \n",
        "\n",
        "    if len(boxes1.shape) > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(len(boxes1.shape)))\n",
        "    if len(boxes2.shape) > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(len(boxes2.shape)))\n",
        "\n",
        "    if len(boxes1.shape) == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
        "    if len(boxes2.shape) == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
        "\n",
        "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"It must be boxes1.shape[1] == boxes2.shape[1] == 4, but it is boxes1.shape[1] == {}, boxes2.shape[1] == {}.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
        "\n",
        "    if coords == 'centroids':\n",
        "        # TODO: Implement a version that uses fewer computation steps (that doesn't need conversion)\n",
        "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2minmax')\n",
        "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2minmax')\n",
        "    elif not (coords in {'minmax', 'corners'}):\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    if coords in {'minmax', 'centroids'}:\n",
        "        intersection = np.maximum(0, np.minimum(boxes1[:,1], boxes2[:,1]) - np.maximum(boxes1[:,0], boxes2[:,0])) * np.maximum(0, np.minimum(boxes1[:,3], boxes2[:,3]) - np.maximum(boxes1[:,2], boxes2[:,2]))\n",
        "        union = (boxes1[:,1] - boxes1[:,0]) * (boxes1[:,3] - boxes1[:,2]) + (boxes2[:,1] - boxes2[:,0]) * (boxes2[:,3] - boxes2[:,2]) - intersection\n",
        "    elif coords == 'corners':\n",
        "        intersection = np.maximum(0, np.minimum(boxes1[:,2], boxes2[:,2]) - np.maximum(boxes1[:,0], boxes2[:,0])) * np.maximum(0, np.minimum(boxes1[:,3], boxes2[:,3]) - np.maximum(boxes1[:,1], boxes2[:,1]))\n",
        "        union = (boxes1[:,2] - boxes1[:,0]) * (boxes1[:,3] - boxes1[:,1]) + (boxes2[:,2] - boxes2[:,0]) * (boxes2[:,3] - boxes2[:,1]) - intersection\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "def convert_coordinates(tensor, start_index, conversion):\n",
        "    \n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] # Set h\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif conversion == 'corners2centroids':\n",
        "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
        "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
        "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] # Set w\n",
        "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] # Set h\n",
        "    elif conversion == 'centroids2corners':\n",
        "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
        "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
        "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
        "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
        "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
        "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
        "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def convert_coordinates2(tensor, start_index, conversion):\n",
        "    \n",
        "    ind = start_index\n",
        "    tensor1 = np.copy(tensor).astype(np.float)\n",
        "    if conversion == 'minmax2centroids':\n",
        "        M = np.array([[0.5, 0. , -1.,  0.],\n",
        "                      [0.5, 0. ,  1.,  0.],\n",
        "                      [0. , 0.5,  0., -1.],\n",
        "                      [0. , 0.5,  0.,  1.]])\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    elif conversion == 'centroids2minmax':\n",
        "        M = np.array([[ 1. , 1. ,  0. , 0. ],\n",
        "                      [ 0. , 0. ,  1. , 1. ],\n",
        "                      [-0.5, 0.5,  0. , 0. ],\n",
        "                      [ 0. , 0. , -0.5, 0.5]]) # The multiplicative inverse of the matrix above\n",
        "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids' and 'centroids2minmax'.\")\n",
        "\n",
        "    return tensor1\n",
        "\n",
        "def greedy_nms(y_pred_decoded, iou_threshold=0.45, coords='corners'):\n",
        "  \n",
        "    y_pred_decoded_nms = []\n",
        "    for batch_item in y_pred_decoded: # For the labels of each batch item...\n",
        "        boxes_left = np.copy(batch_item)\n",
        "        maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "        while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "            maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "            maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "            maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "            boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "            if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "            similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "            boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "        y_pred_decoded_nms.append(np.array(maxima))\n",
        "\n",
        "    return y_pred_decoded_nms\n",
        "\n",
        "def _greedy_nms(predictions, iou_threshold=0.45, coords='corners'):\n",
        "    '''\n",
        "    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n",
        "    function for per-class NMS in `decode_y()`.\n",
        "    '''\n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,0]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,1:], maximum_box[1:], coords=coords) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def _greedy_nms2(predictions, iou_threshold=0.45, coords='corners'):\n",
        "    '''\n",
        "    The same greedy non-maximum suppression algorithm as above, but slightly modified for use as an internal\n",
        "    function in `decode_y2()`.\n",
        "    '''\n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def decode_y(y_pred,\n",
        "             confidence_thresh=0.01,\n",
        "             iou_threshold=0.45,\n",
        "             top_k=200,\n",
        "             input_coords='centroids',\n",
        "             normalize_coords=False,\n",
        "             img_height=None,\n",
        "             img_width=None):\n",
        "   \n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "\n",
        "    y_pred_decoded_raw = np.copy(y_pred[:,:,:-8]) # Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`\n",
        "\n",
        "    if input_coords == 'centroids':\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]] * y_pred[:,:,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= y_pred[:,:,[-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= y_pred[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] += y_pred[:,:,[-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "\n",
        "    if normalize_coords:\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 3: Apply confidence thresholding and non-maximum suppression per class\n",
        "\n",
        "    n_classes = y_pred_decoded_raw.shape[-1] - 4 # The number of classes is the length of the last axis minus the four box coordinates\n",
        "\n",
        "    y_pred_decoded = [] # Store the final predictions in this list\n",
        "    for batch_item in y_pred_decoded_raw: # `batch_item` has shape `[n_boxes, n_classes + 4 coords]`\n",
        "        pred = [] # Store the final predictions for this batch item here\n",
        "        for class_id in range(1, n_classes): # For each class except the background class (which has class ID 0)...\n",
        "            single_class = batch_item[:,[class_id, -4, -3, -2, -1]] # ...keep only the confidences for that class, making this an array of shape `[n_boxes, 5]` and...\n",
        "            threshold_met = single_class[single_class[:,0] > confidence_thresh] # ...keep only those boxes with a confidence above the set threshold.\n",
        "            if threshold_met.shape[0] > 0: # If any boxes made the threshold...\n",
        "                maxima = _greedy_nms(threshold_met, iou_threshold=iou_threshold, coords='corners') # ...perform NMS on them.\n",
        "                maxima_output = np.zeros((maxima.shape[0], maxima.shape[1] + 1)) # Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`\n",
        "                maxima_output[:,0] = class_id # Write the class ID to the first column...\n",
        "                maxima_output[:,1:] = maxima # ...and write the maxima to the other columns...\n",
        "                pred.append(maxima_output) # ...and append the maxima for this class to the list of maxima for this batch item.\n",
        "        # Once we're through with all classes, keep only the `top_k` maxima with the highest scores\n",
        "        if pred: # If there are any predictions left after confidence-thresholding...\n",
        "            pred = np.concatenate(pred, axis=0)\n",
        "            if pred.shape[0] > top_k: # If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...\n",
        "                top_k_indices = np.argpartition(pred[:,1], kth=pred.shape[0]-top_k, axis=0)[pred.shape[0]-top_k:] # ...get the indices of the `top_k` highest-score maxima...\n",
        "                pred = pred[top_k_indices] # ...and keep only those entries of `pred`...\n",
        "        y_pred_decoded.append(pred) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "def decode_y2(y_pred,\n",
        "              confidence_thresh=0.5,\n",
        "              iou_threshold=0.45,\n",
        "              top_k='all',\n",
        "              input_coords='centroids',\n",
        "              normalize_coords=False,\n",
        "              img_height=None,\n",
        "              img_width=None):\n",
        "    \n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the classes from one-hot encoding to their class ID\n",
        "    y_pred_converted = np.copy(y_pred[:,:,-14:-8]) # Slice out the four offset predictions plus two elements whereto we'll write the class IDs and confidences in the next step\n",
        "    y_pred_converted[:,:,0] = np.argmax(y_pred[:,:,:-12], axis=-1) # The indices of the highest confidence values in the one-hot class vectors are the class ID\n",
        "    y_pred_converted[:,:,1] = np.amax(y_pred[:,:,:-12], axis=-1) # Store the confidence values themselves, too\n",
        "\n",
        "    # 2: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "    if input_coords == 'centroids':\n",
        "        y_pred_converted[:,:,[4,5]] = np.exp(y_pred_converted[:,:,[4,5]] * y_pred[:,:,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)\n",
        "        y_pred_converted[:,:,[4,5]] *= y_pred[:,:,[-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)\n",
        "        y_pred_converted[:,:,[2,3]] *= y_pred[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)\n",
        "        y_pred_converted[:,:,[2,3]] += y_pred[:,:,[-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)\n",
        "        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_converted[:,:,[2,3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_converted[:,:,[4,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_converted[:,:,[2,4]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_converted[:,:,[3,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 3: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "    if normalize_coords:\n",
        "        y_pred_converted[:,:,[2,4]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_converted[:,:,[3,5]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 4: Decode our huge `(batch, #boxes, 6)` tensor into a list of length `batch` where each list entry is an array containing only the positive predictions\n",
        "    y_pred_decoded = []\n",
        "    for batch_item in y_pred_converted: # For each image in the batch...\n",
        "        boxes = batch_item[np.nonzero(batch_item[:,0])] # ...get all boxes that don't belong to the background class,...\n",
        "        boxes = boxes[boxes[:,1] >= confidence_thresh] # ...then filter out those positive boxes for which the prediction confidence is too low and after that...\n",
        "        if iou_threshold: # ...if an IoU threshold is set...\n",
        "            boxes = _greedy_nms2(boxes, iou_threshold=iou_threshold, coords='corners') # ...perform NMS on the remaining boxes.\n",
        "        if top_k != 'all' and boxes.shape[0] > top_k: # If we have more than `top_k` results left at this point...\n",
        "            top_k_indices = np.argpartition(boxes[:,1], kth=boxes.shape[0]-top_k, axis=0)[boxes.shape[0]-top_k:] # ...get the indices of the `top_k` highest-scoring boxes...\n",
        "            boxes = boxes[top_k_indices] # ...and keep only those boxes...\n",
        "        y_pred_decoded.append(boxes) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "class SSDBoxEncoder:\n",
        "    \n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 n_classes,\n",
        "                 predictor_sizes,\n",
        "                 min_scale=0.1,\n",
        "                 max_scale=0.9,\n",
        "                 scales=None,\n",
        "                 aspect_ratios_global=[0.5, 1.0, 2.0],\n",
        "                 aspect_ratios_per_layer=None,\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 steps=None,\n",
        "                 offsets=None,\n",
        "                 limit_boxes=False,\n",
        "                 variances=[1.0, 1.0, 1.0, 1.0],\n",
        "                 pos_iou_threshold=0.5,\n",
        "                 neg_iou_threshold=0.3,\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=False):\n",
        "       \n",
        "        predictor_sizes = np.array(predictor_sizes)\n",
        "        if len(predictor_sizes.shape) == 1:\n",
        "            predictor_sizes = np.expand_dims(predictor_sizes, axis=0)\n",
        "\n",
        "        if (min_scale is None or max_scale is None) and scales is None:\n",
        "            raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "\n",
        "        if scales:\n",
        "            if (len(scales) != len(predictor_sizes)+1): # Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`\n",
        "                raise ValueError(\"It must be either scales is None or len(scales) == len(predictor_sizes)+1, but len(scales) == {} and len(predictor_sizes)+1 == {}\".format(len(scales), len(predictor_sizes)+1))\n",
        "            scales = np.array(scales)\n",
        "            if np.any(scales <= 0):\n",
        "                raise ValueError(\"All values in `scales` must be greater than 0, but the passed list of scales is {}\".format(scales))\n",
        "        else: # If no list of scales was passed, we need to make sure that `min_scale` and `max_scale` are valid values.\n",
        "            if not 0 < min_scale <= max_scale:\n",
        "                raise ValueError(\"It must be 0 < min_scale <= max_scale, but it is min_scale = {} and max_scale = {}\".format(min_scale, max_scale))\n",
        "\n",
        "        if not (aspect_ratios_per_layer is None):\n",
        "            if (len(aspect_ratios_per_layer) != len(predictor_sizes)): # Must be two nested `if` statements since `list` and `bool` cannot be combined by `&`\n",
        "                raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == len(predictor_sizes), but len(aspect_ratios_per_layer) == {} and len(predictor_sizes) == {}\".format(len(aspect_ratios_per_layer), len(predictor_sizes)))\n",
        "            for aspect_ratios in aspect_ratios_per_layer:\n",
        "                if np.any(np.array(aspect_ratios) <= 0):\n",
        "                    raise ValueError(\"All aspect ratios must be greater than zero.\")\n",
        "        else:\n",
        "            if (aspect_ratios_global is None):\n",
        "                raise ValueError(\"At least one of `aspect_ratios_global` and `aspect_ratios_per_layer` must not be `None`.\")\n",
        "            if np.any(np.array(aspect_ratios_global) <= 0):\n",
        "                raise ValueError(\"All aspect ratios must be greater than zero.\")\n",
        "\n",
        "        if len(variances) != 4:\n",
        "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        if neg_iou_threshold > pos_iou_threshold:\n",
        "            raise ValueError(\"It cannot be `neg_iou_threshold > pos_iou_threshold`.\")\n",
        "\n",
        "        if not (coords == 'minmax' or coords == 'centroids' or coords == 'corners'):\n",
        "            raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "        if (not (steps is None)) and (len(steps) != len(predictor_sizes)):\n",
        "            raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "        if (not (offsets is None)) and (len(offsets) != len(predictor_sizes)):\n",
        "            raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.n_classes = n_classes + 1\n",
        "        self.predictor_sizes = predictor_sizes\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        if (scales is None):\n",
        "            self.scales = np.linspace(self.min_scale, self.max_scale, len(self.predictor_sizes)+1)\n",
        "        else:\n",
        "            # If a list of scales is given explicitly, we'll use that instead of computing it from `min_scale` and `max_scale`.\n",
        "            self.scales = scales\n",
        "        if (aspect_ratios_per_layer is None):\n",
        "            self.aspect_ratios = [aspect_ratios_global] * len(predictor_sizes)\n",
        "        else:\n",
        "            # If aspect ratios are given per layer, we'll use those.\n",
        "            self.aspect_ratios = aspect_ratios_per_layer\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        if (not steps is None):\n",
        "            self.steps = steps\n",
        "        else:\n",
        "            self.steps = [None] * len(predictor_sizes)\n",
        "        if (not offsets is None):\n",
        "            self.offsets = offsets\n",
        "        else:\n",
        "            self.offsets = [None] * len(predictor_sizes)\n",
        "        self.limit_boxes = limit_boxes\n",
        "        self.variances = variances\n",
        "        self.pos_iou_threshold = pos_iou_threshold\n",
        "        self.neg_iou_threshold = neg_iou_threshold\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "\n",
        "        # Compute the number of boxes per cell.\n",
        "        if aspect_ratios_per_layer:\n",
        "            self.n_boxes = []\n",
        "            for aspect_ratios in aspect_ratios_per_layer:\n",
        "                if (1 in aspect_ratios) & two_boxes_for_ar1:\n",
        "                    self.n_boxes.append(len(aspect_ratios) + 1)\n",
        "                else:\n",
        "                    self.n_boxes.append(len(aspect_ratios))\n",
        "        else:\n",
        "            if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "                self.n_boxes = len(aspect_ratios_global) + 1\n",
        "            else:\n",
        "                self.n_boxes = len(aspect_ratios_global)\n",
        "\n",
        "        # Compute the anchor boxes for all the predictor layers. We only have to do this once\n",
        "        # as the anchor boxes depend only on the model configuration, not on the input data.\n",
        "        # For each conv predictor layer (i.e. for each scale factor)the tensors for that lauer's\n",
        "        # anchor boxes will have the shape `(feature_map_height, feature_map_width, n_boxes, 4)`.\n",
        "        self.boxes_list = [] # This will contain the anchor boxes for each predicotr layer.\n",
        "\n",
        "        self.wh_list_diag = [] # Box widths and heights for each predictor layer\n",
        "        self.steps_diag = [] # Horizontal and vertical distances between any two boxes for each predictor layer\n",
        "        self.offsets_diag = [] # Offsets for each predictor layer\n",
        "        self.centers_diag = [] # Anchor box center points as `(cy, cx)` for each predictor layer\n",
        "\n",
        "        for i in range(len(self.predictor_sizes)):\n",
        "            boxes, center, wh, step, offset = self.generate_anchor_boxes_for_layer(feature_map_size=self.predictor_sizes[i],\n",
        "                                                                                   aspect_ratios=self.aspect_ratios[i],\n",
        "                                                                                   this_scale=self.scales[i],\n",
        "                                                                                   next_scale=self.scales[i+1],\n",
        "                                                                                   this_steps=self.steps[i],\n",
        "                                                                                   this_offsets=self.offsets[i],\n",
        "                                                                                   diagnostics=True)\n",
        "            self.boxes_list.append(boxes)\n",
        "            self.wh_list_diag.append(wh)\n",
        "            self.steps_diag.append(step)\n",
        "            self.offsets_diag.append(offset)\n",
        "            self.centers_diag.append(center)\n",
        "\n",
        "    def generate_anchor_boxes_for_layer(self,\n",
        "                                        feature_map_size,\n",
        "                                        aspect_ratios,\n",
        "                                        this_scale,\n",
        "                                        next_scale,\n",
        "                                        this_steps=None,\n",
        "                                        this_offsets=None,\n",
        "                                        diagnostics=False):\n",
        "       \n",
        "        # Compute box width and height for each aspect ratio.\n",
        "\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(this_scale * next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_width = this_scale * size * np.sqrt(ar)\n",
        "                box_height = this_scale * size / np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "        n_boxes = len(wh_list)\n",
        "\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (this_steps is None):\n",
        "            step_height = self.img_height / feature_map_size[0]\n",
        "            step_width = self.img_width / feature_map_size[1]\n",
        "        else:\n",
        "            if isinstance(this_steps, (list, tuple)) and (len(this_steps) == 2):\n",
        "                step_height = this_steps[0]\n",
        "                step_width = this_steps[1]\n",
        "            elif isinstance(this_steps, (int, float)):\n",
        "                step_height = this_steps\n",
        "                step_width = this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(this_offsets, (list, tuple)) and (len(this_offsets) == 2):\n",
        "                offset_height = this_offsets[0]\n",
        "                offset_width = this_offsets[1]\n",
        "            elif isinstance(this_offsets, (int, float)):\n",
        "                offset_height = this_offsets\n",
        "                offset_width = this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_size[0] - 1) * step_height, feature_map_size[0])\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_size[1] - 1) * step_width, feature_map_size[1])\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_size[0], feature_map_size[1], n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
        "\n",
        "        # Convert `(cx, cy, w, h)` to `(xmin, ymin, xmax, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # If `limit_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.limit_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
        "        if self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids')\n",
        "        elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax')\n",
        "\n",
        "        if diagnostics:\n",
        "            return boxes_tensor, (cy, cx), wh_list, (step_height, step_width), (offset_height, offset_width)\n",
        "        else:\n",
        "            return boxes_tensor\n",
        "\n",
        "    def generate_encode_template(self, batch_size, diagnostics=False):\n",
        "       \n",
        "        # Tile the anchor boxes for each predictor layer across all batch items.\n",
        "        boxes_batch = []\n",
        "        for boxes in self.boxes_list:\n",
        "            # Prepend one dimension to `self.boxes_list` to account for the batch size and tile it along.\n",
        "            # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "            boxes = np.expand_dims(boxes, axis=0)\n",
        "            boxes = np.tile(boxes, (batch_size, 1, 1, 1, 1))\n",
        "\n",
        "            # Now reshape the 5D tensor above into a 3D tensor of shape\n",
        "            # `(batch, feature_map_height * feature_map_width * n_boxes, 4)`. The resulting\n",
        "            # order of the tensor content will be identical to the order obtained from the reshaping operation\n",
        "            # in our Keras model (we're using the Tensorflow backend, and tf.reshape() and np.reshape()\n",
        "            # use the same default index order, which is C-like index ordering)\n",
        "            boxes = np.reshape(boxes, (batch_size, -1, 4))\n",
        "            boxes_batch.append(boxes)\n",
        "\n",
        "        # Concatenate the anchor tensors from the individual layers to one.\n",
        "        boxes_tensor = np.concatenate(boxes_batch, axis=1)\n",
        "\n",
        "        # 3: Create a template tensor to hold the one-hot class encodings of shape `(batch, #boxes, #classes)`\n",
        "        #    It will contain all zeros for now, the classes will be set in the matching process that follows\n",
        "        classes_tensor = np.zeros((batch_size, boxes_tensor.shape[1], self.n_classes))\n",
        "\n",
        "        # 4: Create a tensor to contain the variances. This tensor has the same shape as `boxes_tensor` and simply\n",
        "        #    contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor)\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "\n",
        "        # 4: Concatenate the classes, boxes and variances tensors to get our final template for y_encoded. We also need\n",
        "        #    another tensor of the shape of `boxes_tensor` as a space filler so that `y_encode_template` has the same\n",
        "        #    shape as the SSD model output tensor. The content of this tensor is irrelevant, we'll just use\n",
        "        #    `boxes_tensor` a second time.\n",
        "        y_encode_template = np.concatenate((classes_tensor, boxes_tensor, boxes_tensor, variances_tensor), axis=2)\n",
        "\n",
        "        if diagnostics:\n",
        "            return y_encode_template, self.centers_diag, self.wh_list_diag, self.steps_diag, self.offsets_diag\n",
        "        else:\n",
        "            return y_encode_template\n",
        "\n",
        "    def encode_y(self, ground_truth_labels, diagnostics=False):\n",
        "       \n",
        "\n",
        "        # 1: Generate the template for y_encoded\n",
        "        y_encode_template = self.generate_encode_template(batch_size=len(ground_truth_labels), diagnostics=False)\n",
        "        y_encoded = np.copy(y_encode_template) # We'll write the ground truth box data to this array\n",
        "\n",
        "        # 2: Match the boxes from `ground_truth_labels` to the anchor boxes in `y_encode_template`\n",
        "        #    and for each matched box record the ground truth coordinates in `y_encoded`.\n",
        "        #    Every time there is no match for a anchor box, record `class_id` 0 in `y_encoded` for that anchor box.\n",
        "\n",
        "        class_vector = np.eye(self.n_classes) # An identity matrix that we'll use as one-hot class vectors\n",
        "\n",
        "        for i in range(y_encode_template.shape[0]): # For each batch item...\n",
        "            available_boxes = np.ones((y_encode_template.shape[1])) # 1 for all anchor boxes that are not yet matched to a ground truth box, 0 otherwise\n",
        "            negative_boxes = np.ones((y_encode_template.shape[1])) # 1 for all negative boxes, 0 otherwise\n",
        "            for true_box in ground_truth_labels[i]: # For each ground truth box belonging to the current batch item...\n",
        "                true_box = true_box.astype(np.float)\n",
        "                if abs(true_box[3] - true_box[1] < 0.001) or abs(true_box[4] - true_box[2] < 0.001): continue # Protect ourselves against bad ground truth data: boxes with width or height equal to zero\n",
        "                if self.normalize_coords:\n",
        "                    true_box[[1,3]] /= self.img_width # Normalize xmin and xmax to be within [0,1]\n",
        "                    true_box[[2,4]] /= self.img_height # Normalize ymin and ymax to be within [0,1]\n",
        "                if self.coords == 'centroids':\n",
        "                    true_box = convert_coordinates(true_box, start_index=1, conversion='corners2centroids')\n",
        "                elif self.coords == 'minmax':\n",
        "                    true_box = convert_coordinates(true_box, start_index=1, conversion='corners2minmax')\n",
        "                similarities = iou(y_encode_template[i,:,-12:-8], true_box[1:], coords=self.coords) # The iou similarities for all anchor boxes\n",
        "                negative_boxes[similarities >= self.neg_iou_threshold] = 0 # If a negative box gets an IoU match >= `self.neg_iou_threshold`, it's no longer a valid negative box\n",
        "                similarities *= available_boxes # Filter out anchor boxes which aren't available anymore (i.e. already matched to a different ground truth box)\n",
        "                available_and_thresh_met = np.copy(similarities)\n",
        "                available_and_thresh_met[available_and_thresh_met < self.pos_iou_threshold] = 0 # Filter out anchor boxes which don't meet the iou threshold\n",
        "                assign_indices = np.nonzero(available_and_thresh_met)[0] # Get the indices of the left-over anchor boxes to which we want to assign this ground truth box\n",
        "                if len(assign_indices) > 0: # If we have any matches\n",
        "                    y_encoded[i,assign_indices,:-8] = np.concatenate((class_vector[int(true_box[0])], true_box[1:]), axis=0) # Write the ground truth box coordinates and class to all assigned anchor box positions. Remember that the last four elements of `y_encoded` are just dummy entries.\n",
        "                    available_boxes[assign_indices] = 0 # Make the assigned anchor boxes unavailable for the next ground truth box\n",
        "                else: # If we don't have any matches\n",
        "                    best_match_index = np.argmax(similarities) # Get the index of the best iou match out of all available boxes\n",
        "                    y_encoded[i,best_match_index,:-8] = np.concatenate((class_vector[int(true_box[0])], true_box[1:]), axis=0) # Write the ground truth box coordinates and class to the best match anchor box position\n",
        "                    available_boxes[best_match_index] = 0 # Make the assigned anchor box unavailable for the next ground truth box\n",
        "                    negative_boxes[best_match_index] = 0 # The assigned anchor box is no longer a negative box\n",
        "            # Set the classes of all remaining available anchor boxes to class zero\n",
        "            background_class_indices = np.nonzero(negative_boxes)[0]\n",
        "            y_encoded[i,background_class_indices,0] = 1\n",
        "\n",
        "        # 3: Convert absolute box coordinates to offsets from the anchor boxes and normalize them\n",
        "        if self.coords == 'centroids':\n",
        "            y_encoded[:,:,[-12,-11]] -= y_encode_template[:,:,[-12,-11]] # cx(gt) - cx(anchor), cy(gt) - cy(anchor)\n",
        "            y_encoded[:,:,[-12,-11]] /= y_encode_template[:,:,[-10,-9]] * y_encode_template[:,:,[-4,-3]] # (cx(gt) - cx(anchor)) / w(anchor) / cx_variance, (cy(gt) - cy(anchor)) / h(anchor) / cy_variance\n",
        "            y_encoded[:,:,[-10,-9]] /= y_encode_template[:,:,[-10,-9]] # w(gt) / w(anchor), h(gt) / h(anchor)\n",
        "            y_encoded[:,:,[-10,-9]] = np.log(y_encoded[:,:,[-10,-9]]) / y_encode_template[:,:,[-2,-1]] # ln(w(gt) / w(anchor)) / w_variance, ln(h(gt) / h(anchor)) / h_variance (ln == natural logarithm)\n",
        "        elif self.coords == 'corners':\n",
        "            y_encoded[:,:,-12:-8] -= y_encode_template[:,:,-12:-8] # (gt - anchor) for all four coordinates\n",
        "            y_encoded[:,:,[-12,-10]] /= np.expand_dims(y_encode_template[:,:,-10] - y_encode_template[:,:,-12], axis=-1) # (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)\n",
        "            y_encoded[:,:,[-11,-9]] /= np.expand_dims(y_encode_template[:,:,-9] - y_encode_template[:,:,-11], axis=-1) # (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)\n",
        "            y_encoded[:,:,-12:-8] /= y_encode_template[:,:,-4:] # (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively\n",
        "        else:\n",
        "            y_encoded[:,:,-12:-8] -= y_encode_template[:,:,-12:-8] # (gt - anchor) for all four coordinates\n",
        "            y_encoded[:,:,[-12,-11]] /= np.expand_dims(y_encode_template[:,:,-11] - y_encode_template[:,:,-12], axis=-1) # (xmin(gt) - xmin(anchor)) / w(anchor), (xmax(gt) - xmax(anchor)) / w(anchor)\n",
        "            y_encoded[:,:,[-10,-9]] /= np.expand_dims(y_encode_template[:,:,-9] - y_encode_template[:,:,-10], axis=-1) # (ymin(gt) - ymin(anchor)) / h(anchor), (ymax(gt) - ymax(anchor)) / h(anchor)\n",
        "            y_encoded[:,:,-12:-8] /= y_encode_template[:,:,-4:] # (gt - anchor) / size(anchor) / variance for all four coordinates, where 'size' refers to w and h respectively\n",
        "\n",
        "        if diagnostics:\n",
        "            # Here we'll save the matched anchor boxes (i.e. anchor boxes that were matched to a ground truth box, but keeping the anchor box coordinates).\n",
        "            y_matched_anchors = np.copy(y_encoded)\n",
        "            y_matched_anchors[:,:,-12:-8] = 0 # Keeping the anchor box coordinates means setting the offsets to zero.\n",
        "            return y_encoded, y_matched_anchors\n",
        "        else:\n",
        "            return y_encoded\n",
        "\n",
        "################################################################################\n",
        "# Debugging tools, not relevant for normal use\n",
        "################################################################################\n",
        "\n",
        "# The functions below are for debugging, so you won't normally need them. That is,\n",
        "# unless you need to debug your model, of course.\n",
        "\n",
        "def decode_y_debug(y_pred,\n",
        "                   confidence_thresh=0.01,\n",
        "                   iou_threshold=0.45,\n",
        "                   top_k=200,\n",
        "                   input_coords='centroids',\n",
        "                   normalize_coords=False,\n",
        "                   img_height=None,\n",
        "                   img_width=None,\n",
        "                   variance_encoded_in_target=False):\n",
        "  \n",
        "    if normalize_coords and ((img_height is None) or (img_width is None)):\n",
        "        raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
        "\n",
        "    # 1: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates\n",
        "\n",
        "    y_pred_decoded_raw = np.copy(y_pred[:,:,:-8]) # Slice out the classes and the four offsets, throw away the anchor coordinates and variances, resulting in a tensor of shape `[batch, n_boxes, n_classes + 4 coordinates]`\n",
        "\n",
        "    if input_coords == 'centroids':\n",
        "        if variance_encoded_in_target:\n",
        "            # Decode the predicted box center x and y coordinates.\n",
        "            y_pred_decoded_raw[:,:,[-4,-3]] = y_pred_decoded_raw[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] + y_pred[:,:,[-8,-7]]\n",
        "            # Decode the predicted box width and heigt.\n",
        "            y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]]) * y_pred[:,:,[-6,-5]]\n",
        "        else:\n",
        "            # Decode the predicted box center x and y coordinates.\n",
        "            y_pred_decoded_raw[:,:,[-4,-3]] = y_pred_decoded_raw[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] * y_pred[:,:,[-4,-3]] + y_pred[:,:,[-8,-7]]\n",
        "            # Decode the predicted box width and heigt.\n",
        "            y_pred_decoded_raw[:,:,[-2,-1]] = np.exp(y_pred_decoded_raw[:,:,[-2,-1]] * y_pred[:,:,[-2,-1]]) * y_pred[:,:,[-6,-5]]\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='centroids2corners')\n",
        "    elif input_coords == 'minmax':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-2,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "        y_pred_decoded_raw = convert_coordinates(y_pred_decoded_raw, start_index=-4, conversion='minmax2corners')\n",
        "    elif input_coords == 'corners':\n",
        "        y_pred_decoded_raw[:,:,-4:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)\n",
        "        y_pred_decoded_raw[:,:,-4:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates\n",
        "    else:\n",
        "        raise ValueError(\"Unexpected value for `input_coords`. Supported input coordinate formats are 'minmax', 'corners' and 'centroids'.\")\n",
        "\n",
        "    # 2: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that\n",
        "\n",
        "    if normalize_coords:\n",
        "        y_pred_decoded_raw[:,:,[-4,-2]] *= img_width # Convert xmin, xmax back to absolute coordinates\n",
        "        y_pred_decoded_raw[:,:,[-3,-1]] *= img_height # Convert ymin, ymax back to absolute coordinates\n",
        "\n",
        "    # 3: For each batch item, prepend each box's internal index to its coordinates.\n",
        "\n",
        "    y_pred_decoded_raw2 = np.zeros((y_pred_decoded_raw.shape[0], y_pred_decoded_raw.shape[1], y_pred_decoded_raw.shape[2] + 1)) # Expand the last axis by one.\n",
        "    y_pred_decoded_raw2[:,:,1:] = y_pred_decoded_raw\n",
        "    y_pred_decoded_raw2[:,:,0] = np.arange(y_pred_decoded_raw.shape[1]) # Put the box indices as the first element for each box via broadcasting.\n",
        "    y_pred_decoded_raw = y_pred_decoded_raw2\n",
        "\n",
        "    # 4: Apply confidence thresholding and non-maximum suppression per class\n",
        "\n",
        "    n_classes = y_pred_decoded_raw.shape[-1] - 5 # The number of classes is the length of the last axis minus the four box coordinates and minus the index\n",
        "\n",
        "    y_pred_decoded = [] # Store the final predictions in this list\n",
        "    for batch_item in y_pred_decoded_raw: # `batch_item` has shape `[n_boxes, n_classes + 4 coords]`\n",
        "        pred = [] # Store the final predictions for this batch item here\n",
        "        for class_id in range(1, n_classes): # For each class except the background class (which has class ID 0)...\n",
        "            single_class = batch_item[:,[0, class_id + 1, -4, -3, -2, -1]] # ...keep only the confidences for that class, making this an array of shape `[n_boxes, 6]` and...\n",
        "            threshold_met = single_class[single_class[:,1] > confidence_thresh] # ...keep only those boxes with a confidence above the set threshold.\n",
        "            if threshold_met.shape[0] > 0: # If any boxes made the threshold...\n",
        "                maxima = _greedy_nms_debug(threshold_met, iou_threshold=iou_threshold, coords='corners') # ...perform NMS on them.\n",
        "                maxima_output = np.zeros((maxima.shape[0], maxima.shape[1] + 1)) # Expand the last dimension by one element to have room for the class ID. This is now an arrray of shape `[n_boxes, 6]`\n",
        "                maxima_output[:,0] = maxima[:,0] # Write the box index to the first column...\n",
        "                maxima_output[:,1] = class_id # ...and write the class ID to the second column...\n",
        "                maxima_output[:,2:] = maxima[:,1:] # ...and write the rest of the maxima data to the other columns...\n",
        "                pred.append(maxima_output) # ...and append the maxima for this class to the list of maxima for this batch item.\n",
        "        # Once we're through with all classes, keep only the `top_k` maxima with the highest scores\n",
        "        pred = np.concatenate(pred, axis=0)\n",
        "        if pred.shape[0] > top_k: # If we have more than `top_k` results left at this point, otherwise there is nothing to filter,...\n",
        "            top_k_indices = np.argpartition(pred[:,2], kth=pred.shape[0]-top_k, axis=0)[pred.shape[0]-top_k:] # ...get the indices of the `top_k` highest-score maxima...\n",
        "            pred = pred[top_k_indices] # ...and keep only those entries of `pred`...\n",
        "        y_pred_decoded.append(pred) # ...and now that we're done, append the array of final predictions for this batch item to the output list\n",
        "\n",
        "    return y_pred_decoded\n",
        "\n",
        "def _greedy_nms_debug(predictions, iou_threshold=0.45, coords='corners'):\n",
        "    \n",
        "    boxes_left = np.copy(predictions)\n",
        "    maxima = [] # This is where we store the boxes that make it through the non-maximum suppression\n",
        "    while boxes_left.shape[0] > 0: # While there are still boxes left to compare...\n",
        "        maximum_index = np.argmax(boxes_left[:,1]) # ...get the index of the next box with the highest confidence...\n",
        "        maximum_box = np.copy(boxes_left[maximum_index]) # ...copy that box and...\n",
        "        maxima.append(maximum_box) # ...append it to `maxima` because we'll definitely keep it\n",
        "        boxes_left = np.delete(boxes_left, maximum_index, axis=0) # Now remove the maximum box from `boxes_left`\n",
        "        if boxes_left.shape[0] == 0: break # If there are no boxes left after this step, break. Otherwise...\n",
        "        similarities = iou(boxes_left[:,2:], maximum_box[2:], coords=coords) # ...compare (IoU) the other left over boxes to the maximum box...\n",
        "        boxes_left = boxes_left[similarities <= iou_threshold] # ...so that we can remove the ones that overlap too much with the maximum box\n",
        "    return np.array(maxima)\n",
        "\n",
        "def get_num_boxes_per_pred_layer(predictor_sizes, aspect_ratios, two_boxes_for_ar1):\n",
        "    \n",
        "    num_boxes_per_pred_layer = []\n",
        "    for i in range(len(predictor_sizes)):\n",
        "        if two_boxes_for_ar1:\n",
        "            num_boxes_per_pred_layer.append(predictor_sizes[i][0] * predictor_sizes[i][1] * (len(aspect_ratios[i]) + 1))\n",
        "        else:\n",
        "            num_boxes_per_pred_layer.append(predictor_sizes[i][0] * predictor_sizes[i][1] * len(aspect_ratios[i]))\n",
        "    return num_boxes_per_pred_layer\n",
        "\n",
        "def get_pred_layers(y_pred_decoded, num_boxes_per_pred_layer):\n",
        "    \n",
        "    pred_layers_all = []\n",
        "    cum_boxes_per_pred_layer = np.cumsum(num_boxes_per_pred_layer)\n",
        "    for batch_item in y_pred_decoded:\n",
        "        pred_layers = []\n",
        "        for prediction in batch_item:\n",
        "            if (prediction[0] < 0) or (prediction[0] >= cum_boxes_per_pred_layer[-1]):\n",
        "                raise ValueError(\"Box index is out of bounds of the possible indices as given by the values in `num_boxes_per_pred_layer`.\")\n",
        "            for i in range(len(cum_boxes_per_pred_layer)):\n",
        "                if prediction[0] < cum_boxes_per_pred_layer[i]:\n",
        "                    pred_layers.append(i)\n",
        "                    break\n",
        "        pred_layers_all.append(pred_layers)\n",
        "    return pred_layers_all\n",
        "    \n",
        "\n",
        "#ssd_box_encode_decode_utils completete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fa4vmhFRNdHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keras_layer_AnchorBoxes\n",
        "\n",
        "class AnchorBoxes(Layer):\n",
        "   \n",
        "\n",
        "    def __init__(self,\n",
        "                 img_height,\n",
        "                 img_width,\n",
        "                 this_scale,\n",
        "                 next_scale,\n",
        "                 aspect_ratios=[0.5, 1.0, 2.0],\n",
        "                 two_boxes_for_ar1=True,\n",
        "                 this_steps=None,\n",
        "                 this_offsets=None,\n",
        "                 limit_boxes=True,\n",
        "                 variances=[1.0, 1.0, 1.0, 1.0],\n",
        "                 coords='centroids',\n",
        "                 normalize_coords=False,\n",
        "                 **kwargs):\n",
        "       \n",
        "        if K.backend() != 'tensorflow':\n",
        "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
        "\n",
        "        if (this_scale < 0) or (next_scale < 0) or (this_scale > 1):\n",
        "            raise ValueError(\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\".format(this_scale, next_scale))\n",
        "\n",
        "        if len(variances) != 4:\n",
        "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "        variances = np.array(variances)\n",
        "        if np.any(variances <= 0):\n",
        "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.this_scale = this_scale\n",
        "        self.next_scale = next_scale\n",
        "        self.aspect_ratios = aspect_ratios\n",
        "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
        "        self.this_steps = this_steps\n",
        "        self.this_offsets = this_offsets\n",
        "        self.limit_boxes = limit_boxes\n",
        "        self.variances = variances\n",
        "        self.coords = coords\n",
        "        self.normalize_coords = normalize_coords\n",
        "        # Compute the number of boxes per cell\n",
        "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
        "            self.n_boxes = len(aspect_ratios) + 1\n",
        "        else:\n",
        "            self.n_boxes = len(aspect_ratios)\n",
        "        super(AnchorBoxes, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        super(AnchorBoxes, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "       \n",
        "\n",
        "        # Compute box width and height for each aspect ratio\n",
        "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
        "        size = min(self.img_height, self.img_width)\n",
        "        # Compute the box widths and and heights for all aspect ratios\n",
        "        wh_list = []\n",
        "        for ar in self.aspect_ratios:\n",
        "            if (ar == 1):\n",
        "                # Compute the regular anchor box for aspect ratio 1.\n",
        "                box_height = box_width = self.this_scale * size\n",
        "                wh_list.append((box_width, box_height))\n",
        "                if self.two_boxes_for_ar1:\n",
        "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
        "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
        "                    wh_list.append((box_width, box_height))\n",
        "            else:\n",
        "                box_height = self.this_scale * size / np.sqrt(ar)\n",
        "                box_width = self.this_scale * size * np.sqrt(ar)\n",
        "                wh_list.append((box_width, box_height))\n",
        "        wh_list = np.array(wh_list)\n",
        "\n",
        "        # We need the shape of the input tensor\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = x._keras_shape\n",
        "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = x._keras_shape\n",
        "\n",
        "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
        "\n",
        "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
        "        if (self.this_steps is None):\n",
        "            step_height = self.img_height / feature_map_height\n",
        "            step_width = self.img_width / feature_map_width\n",
        "        else:\n",
        "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
        "                step_height = self.this_steps[0]\n",
        "                step_width = self.this_steps[1]\n",
        "            elif isinstance(self.this_steps, (int, float)):\n",
        "                step_height = self.this_steps\n",
        "                step_width = self.this_steps\n",
        "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
        "        if (self.this_offsets is None):\n",
        "            offset_height = 0.5\n",
        "            offset_width = 0.5\n",
        "        else:\n",
        "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
        "                offset_height = self.this_offsets[0]\n",
        "                offset_width = self.this_offsets[1]\n",
        "            elif isinstance(self.this_offsets, (int, float)):\n",
        "                offset_height = self.this_offsets\n",
        "                offset_width = self.this_offsets\n",
        "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
        "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
        "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
        "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
        "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
        "\n",
        "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        # where the last dimension will contain `(cx, cy, w, h)`\n",
        "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n",
        "\n",
        "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
        "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
        "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
        "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
        "\n",
        "        # Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\n",
        "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
        "\n",
        "        # If `limit_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
        "        if self.limit_boxes:\n",
        "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
        "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
        "            x_coords[x_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
        "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
        "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
        "            y_coords[y_coords < 0] = 0\n",
        "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
        "\n",
        "        # If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
        "        if self.normalize_coords:\n",
        "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
        "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
        "\n",
        "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
        "        if self.coords == 'centroids':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids')\n",
        "        elif self.coords == 'minmax':\n",
        "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
        "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax')\n",
        "\n",
        "        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
        "        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
        "        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
        "        variances_tensor += self.variances # Long live broadcasting\n",
        "        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
        "\n",
        "        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
        "        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
        "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
        "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
        "\n",
        "        return boxes_tensor\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
        "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
        "            batch_size, feature_map_channels, feature_map_height, feature_map_width = input_shape\n",
        "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'img_height': self.img_height,\n",
        "            'img_width': self.img_width,\n",
        "            'this_scale': self.this_scale,\n",
        "            'next_scale': self.next_scale,\n",
        "            'aspect_ratios': list(self.aspect_ratios),\n",
        "            'two_boxes_for_ar1': self.two_boxes_for_ar1,\n",
        "            'limit_boxes': self.limit_boxes,\n",
        "            'variances': list(self.variances),\n",
        "            'coords': self.coords,\n",
        "            'normalize_coords': self.normalize_coords\n",
        "        }\n",
        "        base_config = super(AnchorBoxes, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "#keras_layer_AnchorBoxes compelete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPhpssPqNdLO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keras_ssd7\n",
        "\n",
        "def build_model(image_size,\n",
        "                n_classes,\n",
        "                l2_regularization=0.0,\n",
        "                min_scale=0.1,\n",
        "                max_scale=0.9,\n",
        "                scales=None,\n",
        "                aspect_ratios_global=[0.5, 1.0, 2.0],\n",
        "                aspect_ratios_per_layer=None,\n",
        "                two_boxes_for_ar1=True,\n",
        "                steps=None,\n",
        "                offsets=None,\n",
        "                limit_boxes=True,\n",
        "                variances=[1.0, 1.0, 1.0, 1.0],\n",
        "                coords='centroids',\n",
        "                normalize_coords=False,\n",
        "                subtract_mean=None,\n",
        "                divide_by_stddev=None,\n",
        "                swap_channels=False,\n",
        "                return_predictor_sizes=False):\n",
        "   \n",
        "        \n",
        "\n",
        "    n_predictor_layers = 4 \n",
        "\n",
        "    n_classes += 1 \n",
        "    \n",
        "    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n",
        "        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n",
        "    if aspect_ratios_per_layer:\n",
        "        if len(aspect_ratios_per_layer) != n_predictor_layers:\n",
        "            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n",
        "\n",
        "    if (min_scale is None or max_scale is None) and scales is None:\n",
        "        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
        "    if scales:\n",
        "        if len(scales) != n_predictor_layers+1:\n",
        "            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n",
        "    else: \n",
        "        scales = np.linspace(min_scale, max_scale, n_predictor_layers+1)\n",
        "\n",
        "    if len(variances) != 4: \n",
        "        raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
        "    variances = np.array(variances)\n",
        "    if np.any(variances <= 0):\n",
        "        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
        "\n",
        "    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
        "\n",
        "    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n",
        "        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
        "\n",
        "    \n",
        "    if aspect_ratios_per_layer:\n",
        "        aspect_ratios = aspect_ratios_per_layer\n",
        "    else:\n",
        "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
        "\n",
        "    \n",
        "    if aspect_ratios_per_layer:\n",
        "        n_boxes = []\n",
        "        for ar in aspect_ratios_per_layer:\n",
        "            if (1 in ar) & two_boxes_for_ar1:\n",
        "                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n",
        "            else:\n",
        "                n_boxes.append(len(ar))\n",
        "    else: \n",
        "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
        "            n_boxes = len(aspect_ratios_global) + 1\n",
        "        else:\n",
        "            n_boxes = len(aspect_ratios_global)\n",
        "        n_boxes = [n_boxes] * n_predictor_layers\n",
        "\n",
        "    if steps is None:\n",
        "        steps = [None] * n_predictor_layers\n",
        "    if offsets is None:\n",
        "        offsets = [None] * n_predictor_layers\n",
        "\n",
        "    l2_reg = l2_regularization\n",
        "\n",
        "\n",
        "    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
        "\n",
        "    \n",
        "\n",
        "    x = Input(shape=(img_height, img_width, img_channels))\n",
        "\n",
        "    \n",
        "    x1 = Lambda(lambda z: z,\n",
        "                output_shape=(img_height, img_width, img_channels),\n",
        "                name='idendity_layer')(x)\n",
        "    if not (subtract_mean is None):\n",
        "        x1 = Lambda(lambda z: z - np.array(subtract_mean),\n",
        "                   output_shape=(img_height, img_width, img_channels),\n",
        "                   name='input_mean_normalization')(x1)\n",
        "    if not (divide_by_stddev is None):\n",
        "        x1 = Lambda(lambda z: z / np.array(divide_by_stddev),\n",
        "                   output_shape=(img_height, img_width, img_channels),\n",
        "                   name='input_stddev_normalization')(x1)\n",
        "    if swap_channels and (img_channels == 3):\n",
        "        x1 = Lambda(lambda z: z[...,::-1],\n",
        "                   output_shape=(img_height, img_width, img_channels),\n",
        "                   name='input_channel_swap')(x1)\n",
        "\n",
        "    conv1 = Conv2D(32, (5, 5), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1')(x1)\n",
        "    conv1 = BatchNormalization(axis=3, momentum=0.99, name='bn1')(conv1) # Tensorflow uses filter format [filter_height, filter_width, in_channels, out_channels], hence axis = 3\n",
        "    conv1 = ELU(name='elu1')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
        "\n",
        "    conv2 = Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2')(pool1)\n",
        "    conv2 = BatchNormalization(axis=3, momentum=0.99, name='bn2')(conv2)\n",
        "    conv2 = ELU(name='elu2')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2), name='pool2')(conv2)\n",
        "\n",
        "    conv3 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3')(pool2)\n",
        "    conv3 = BatchNormalization(axis=3, momentum=0.99, name='bn3')(conv3)\n",
        "    conv3 = ELU(name='elu3')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2), name='pool3')(conv3)\n",
        "\n",
        "    conv4 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4')(pool3)\n",
        "    conv4 = BatchNormalization(axis=3, momentum=0.99, name='bn4')(conv4)\n",
        "    conv4 = ELU(name='elu4')(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2), name='pool4')(conv4)\n",
        "\n",
        "    conv5 = Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5')(pool4)\n",
        "    conv5 = BatchNormalization(axis=3, momentum=0.99, name='bn5')(conv5)\n",
        "    conv5 = ELU(name='elu5')(conv5)\n",
        "    pool5 = MaxPooling2D(pool_size=(2, 2), name='pool5')(conv5)\n",
        "\n",
        "    conv6 = Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6')(pool5)\n",
        "    conv6 = BatchNormalization(axis=3, momentum=0.99, name='bn6')(conv6)\n",
        "    conv6 = ELU(name='elu6')(conv6)\n",
        "    pool6 = MaxPooling2D(pool_size=(2, 2), name='pool6')(conv6)\n",
        "\n",
        "    conv7 = Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7')(pool6)\n",
        "    conv7 = BatchNormalization(axis=3, momentum=0.99, name='bn7')(conv7)\n",
        "    conv7 = ELU(name='elu7')(conv7)\n",
        "\n",
        "    \n",
        "    classes4 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='classes4')(conv4)\n",
        "    classes5 = Conv2D(n_boxes[1] * n_classes, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='classes5')(conv5)\n",
        "    classes6 = Conv2D(n_boxes[2] * n_classes, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='classes6')(conv6)\n",
        "    classes7 = Conv2D(n_boxes[3] * n_classes, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='classes7')(conv7)\n",
        "    \n",
        "    boxes4 = Conv2D(n_boxes[0] * 4, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='boxes4')(conv4)\n",
        "    boxes5 = Conv2D(n_boxes[1] * 4, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='boxes5')(conv5)\n",
        "    boxes6 = Conv2D(n_boxes[2] * 4, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='boxes6')(conv6)\n",
        "    boxes7 = Conv2D(n_boxes[3] * 4, (3, 3), strides=(1, 1), padding=\"valid\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='boxes7')(conv7)\n",
        "\n",
        "    \n",
        "    anchors4 = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios[0],\n",
        "                           two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[0], this_offsets=offsets[0],\n",
        "                           limit_boxes=limit_boxes, variances=variances, coords=coords, normalize_coords=normalize_coords, name='anchors4')(boxes4)\n",
        "    anchors5 = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios[1],\n",
        "                           two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[1], this_offsets=offsets[1],\n",
        "                           limit_boxes=limit_boxes, variances=variances, coords=coords, normalize_coords=normalize_coords, name='anchors5')(boxes5)\n",
        "    anchors6 = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios[2],\n",
        "                           two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[2], this_offsets=offsets[2],\n",
        "                           limit_boxes=limit_boxes, variances=variances, coords=coords, normalize_coords=normalize_coords, name='anchors6')(boxes6)\n",
        "    anchors7 = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios[3],\n",
        "                           two_boxes_for_ar1=two_boxes_for_ar1, this_steps=steps[3], this_offsets=offsets[3],\n",
        "                           limit_boxes=limit_boxes, variances=variances, coords=coords, normalize_coords=normalize_coords, name='anchors7')(boxes7)\n",
        "\n",
        "   \n",
        "    classes4_reshaped = Reshape((-1, n_classes), name='classes4_reshape')(classes4)\n",
        "    classes5_reshaped = Reshape((-1, n_classes), name='classes5_reshape')(classes5)\n",
        "    classes6_reshaped = Reshape((-1, n_classes), name='classes6_reshape')(classes6)\n",
        "    classes7_reshaped = Reshape((-1, n_classes), name='classes7_reshape')(classes7)\n",
        "    \n",
        "    boxes4_reshaped = Reshape((-1, 4), name='boxes4_reshape')(boxes4)\n",
        "    boxes5_reshaped = Reshape((-1, 4), name='boxes5_reshape')(boxes5)\n",
        "    boxes6_reshaped = Reshape((-1, 4), name='boxes6_reshape')(boxes6)\n",
        "    boxes7_reshaped = Reshape((-1, 4), name='boxes7_reshape')(boxes7)\n",
        "   \n",
        "    anchors4_reshaped = Reshape((-1, 8), name='anchors4_reshape')(anchors4)\n",
        "    anchors5_reshaped = Reshape((-1, 8), name='anchors5_reshape')(anchors5)\n",
        "    anchors6_reshaped = Reshape((-1, 8), name='anchors6_reshape')(anchors6)\n",
        "    anchors7_reshaped = Reshape((-1, 8), name='anchors7_reshape')(anchors7)\n",
        "\n",
        "    \n",
        "    classes_concat = Concatenate(axis=1, name='classes_concat')([classes4_reshaped,\n",
        "                                                                 classes5_reshaped,\n",
        "                                                                 classes6_reshaped,\n",
        "                                                                 classes7_reshaped])\n",
        "\n",
        "    \n",
        "    boxes_concat = Concatenate(axis=1, name='boxes_concat')([boxes4_reshaped,\n",
        "                                                             boxes5_reshaped,\n",
        "                                                             boxes6_reshaped,\n",
        "                                                             boxes7_reshaped])\n",
        "\n",
        "    \n",
        "    anchors_concat = Concatenate(axis=1, name='anchors_concat')([anchors4_reshaped,\n",
        "                                                                 anchors5_reshaped,\n",
        "                                                                 anchors6_reshaped,\n",
        "                                                                 anchors7_reshaped])\n",
        "\n",
        "   \n",
        "    classes_softmax = Activation('softmax', name='classes_softmax')(classes_concat)\n",
        "\n",
        "    predictions = Concatenate(axis=2, name='predictions')([classes_softmax, boxes_concat, anchors_concat])\n",
        "\n",
        "    model = Model(inputs=x, outputs=predictions)\n",
        "\n",
        "    if return_predictor_sizes:\n",
        "       \n",
        "        predictor_sizes = np.array([classes4._keras_shape[1:3],\n",
        "                                    classes5._keras_shape[1:3],\n",
        "                                    classes6._keras_shape[1:3],\n",
        "                                    classes7._keras_shape[1:3]])\n",
        "        return model, predictor_sizes\n",
        "    else:\n",
        "        return model\n",
        "\n",
        "#keras_ssd7 compelete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3p8I83DSNdOz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keras_ssd_loss\n",
        "\n",
        "class SSDLoss:\n",
        "    \n",
        "    def __init__(self,\n",
        "                 neg_pos_ratio=3,\n",
        "                 n_neg_min=0,\n",
        "                 alpha=1.0):\n",
        "    \n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.n_neg_min = n_neg_min\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def smooth_L1_loss(self, y_true, y_pred):\n",
        "      \n",
        "        absolute_loss = tf.abs(y_true - y_pred)\n",
        "        square_loss = 0.5 * (y_true - y_pred)**2\n",
        "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
        "        return tf.reduce_sum(l1_loss, axis=-1)\n",
        "\n",
        "    def log_loss(self, y_true, y_pred):\n",
        "       \n",
        "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
        "        y_pred = tf.maximum(y_pred, 1e-15)\n",
        "        # Compute the log loss\n",
        "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
        "        return log_loss\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \n",
        "        self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
        "        self.n_neg_min = tf.constant(self.n_neg_min)\n",
        "        self.alpha = tf.constant(self.alpha)\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
        "        n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell\n",
        "\n",
        "       \n",
        "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)\n",
        "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)\n",
        "\n",
        "        \n",
        "        negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
        "        positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
        "\n",
        "        \n",
        "        n_positive = tf.reduce_sum(positives)\n",
        "\n",
        "        \n",
        "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "        \n",
        "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
        "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
        "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
        "\n",
        "       \n",
        "        def f1():\n",
        "            return tf.zeros([batch_size])\n",
        "       \n",
        "        def f2():\n",
        "            \n",
        "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
        "          \n",
        "            values, indices = tf.nn.top_k(neg_class_loss_all_1D, n_negative_keep, False) # We don't need sorting\n",
        "           \n",
        "            negatives_keep = tf.scatter_nd(tf.expand_dims(indices, axis=1), updates=tf.ones_like(indices, dtype=tf.int32), shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
        "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
        "           \n",
        "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
        "            return neg_class_loss\n",
        "\n",
        "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
        "\n",
        "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
        "\n",
        "        \n",
        "\n",
        "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
        "\n",
        "       \n",
        "\n",
        "        total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
        "        \n",
        "        total_loss *= tf.to_float(batch_size)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "#keras_ssd7 compelete\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZkL7DjVNdXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#keras_layer_L2Normalization \n",
        "\n",
        "class L2Normalization(Layer):\n",
        "  \n",
        "    def __init__(self, gamma_init=20, **kwargs):\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            self.axis = 3\n",
        "        else:\n",
        "            self.axis = 1\n",
        "        self.gamma_init = gamma_init\n",
        "        super(L2Normalization, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
        "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
        "        self.trainable_weights = [self.gamma]\n",
        "        super(L2Normalization, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output = K.l2_normalize(x, self.axis)\n",
        "        output *= self.gamma\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'gamma_init': self.gamma_init\n",
        "        }\n",
        "        base_config = super(L2Normalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "#keras_layer_L2Normalization compelete\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "im03UuBeNdaJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ssd_batch_generator\n",
        "\n",
        "try:\n",
        "    import json\n",
        "except ImportError:\n",
        "    warnings.warn(\"'json' module is missing. The JSON-parser will be unavailable.\")\n",
        "try:\n",
        "    from bs4 import BeautifulSoup\n",
        "except ImportError:\n",
        "    warnings.warn(\"'BeautifulSoup' module is missing. The XML-parser will be unavailable.\")\n",
        "try:\n",
        "    import pickle\n",
        "except ImportError:\n",
        "    warnings.warn(\"'pickle' module is missing. You won't be able to save parsed file lists and annotations as pickled files.\")\n",
        "\n",
        "# Image processing functions used by the generator to perform the following image manipulations:\n",
        "# - Translation\n",
        "# - Horizontal flip\n",
        "# - Scaling\n",
        "# - Brightness change\n",
        "# - Histogram contrast equalization\n",
        "\n",
        "def _translate(image, horizontal=(0,40), vertical=(0,10)):\n",
        "   \n",
        "    rows,cols,ch = image.shape\n",
        "\n",
        "    x = np.random.randint(horizontal[0], horizontal[1]+1)\n",
        "    y = np.random.randint(vertical[0], vertical[1]+1)\n",
        "    x_shift = random.choice([-x, x])\n",
        "    y_shift = random.choice([-y, y])\n",
        "\n",
        "    M = np.float32([[1,0,x_shift],[0,1,y_shift]])\n",
        "    return cv2.warpAffine(image, M, (cols, rows)), x_shift, y_shift\n",
        "\n",
        "def _flip(image, orientation='horizontal'):\n",
        "   \n",
        "    if orientation == 'horizontal':\n",
        "        return cv2.flip(image, 1)\n",
        "    else:\n",
        "        return cv2.flip(image, 0)\n",
        "\n",
        "def _scale(image, min=0.9, max=1.1):\n",
        "    \n",
        "\n",
        "    rows,cols,ch = image.shape\n",
        "\n",
        "    #Randomly select a scaling factor from the range passed.\n",
        "    scale = np.random.uniform(min, max)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cols/2,rows/2), 0, scale)\n",
        "    return cv2.warpAffine(image, M, (cols, rows)), M, scale\n",
        "\n",
        "def _brightness(image, min=0.5, max=2.0):\n",
        "    \n",
        "    hsv = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    random_br = np.random.uniform(min,max)\n",
        "\n",
        "   \n",
        "    mask = hsv[:,:,2] * random_br > 255\n",
        "    v_channel = np.where(mask, 255, hsv[:,:,2] * random_br)\n",
        "    hsv[:,:,2] = v_channel\n",
        "\n",
        "    return cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
        "\n",
        "def histogram_eq(image):\n",
        "   \n",
        "\n",
        "    image1 = np.copy(image)\n",
        "\n",
        "    image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    image1[:,:,2] = cv2.equalizeHist(image1[:,:,2])\n",
        "\n",
        "    image1 = cv2.cvtColor(image1, cv2.COLOR_HSV2RGB)\n",
        "\n",
        "    return image1\n",
        "\n",
        "class BatchGenerator:\n",
        " \n",
        "\n",
        "    def __init__(self,\n",
        "                 box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'],\n",
        "                 filenames=None,\n",
        "                 filenames_type='text',\n",
        "                 images_dir=None,\n",
        "                 labels=None,\n",
        "                 image_ids=None):\n",
        "        \n",
        "        self.box_output_format = box_output_format\n",
        "\n",
        "        \n",
        "\n",
        "        if not filenames is None:\n",
        "            if isinstance(filenames, (list, tuple)):\n",
        "                self.filenames = filenames\n",
        "            elif isinstance(filenames, str):\n",
        "                with open(filenames, 'rb') as f:\n",
        "                    if filenames_type == 'pickle':\n",
        "                        self.filenames = pickle.load(f)\n",
        "                    elif filenames_type == 'text':\n",
        "                        self.filenames = [os.path.join(images_dir, line.strip()) for line in f]\n",
        "                    else:\n",
        "                        raise ValueError(\"`filenames_type` can be either 'text' or 'pickle'.\")\n",
        "            else:\n",
        "                raise ValueError(\"`filenames` must be either a Python list/tuple or a string representing a filepath (to a pickled or text file). The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.filenames = []\n",
        "\n",
        "        if not labels is None:\n",
        "            if isinstance(labels, str):\n",
        "                with open(labels, 'rb') as f:\n",
        "                    self.labels = pickle.load(f)\n",
        "            elif isinstance(labels, (list, tuple)):\n",
        "                self.labels = labels\n",
        "            else:\n",
        "                raise ValueError(\"`labels` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.labels = None\n",
        "\n",
        "        if not image_ids is None:\n",
        "            if isinstance(image_ids, str):\n",
        "                with open(image_ids, 'rb') as f:\n",
        "                    self.image_ids = pickle.load(f)\n",
        "            elif isinstance(image_ids, (list, tuple)):\n",
        "                self.image_ids = image_ids\n",
        "            else:\n",
        "                raise ValueError(\"`image_ids` must be either a Python list/tuple or a string representing the path to a pickled file containing a list/tuple. The value you passed is neither of the two.\")\n",
        "        else:\n",
        "            self.image_ids = None\n",
        "\n",
        "    def parse_csv(self,\n",
        "                  images_dir,\n",
        "                  labels_filename,\n",
        "                  input_format,\n",
        "                  include_classes='all',\n",
        "                  random_sample=False,\n",
        "                  ret=False):\n",
        "       \n",
        "\n",
        "        # Set class members.\n",
        "        self.images_dir = images_dir\n",
        "        self.labels_filename = labels_filename\n",
        "        self.input_format = input_format\n",
        "        self.include_classes = include_classes\n",
        "\n",
        "        # Before we begin, make sure that we have a labels_filename and an input_format\n",
        "        if self.labels_filename is None or self.input_format is None:\n",
        "            raise ValueError(\"`labels_filename` and/or `input_format` have not been set yet. You need to pass them as arguments.\")\n",
        "\n",
        "        # Erase data that might have been parsed before\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        # First, just read in the CSV file lines and sort them.\n",
        "\n",
        "        data = []\n",
        "\n",
        "        with open(self.labels_filename, newline='') as csvfile:\n",
        "            csvread = csv.reader(csvfile, delimiter=',')\n",
        "            next(csvread) # Skip the header row.\n",
        "            for row in csvread: # For every line (i.e for every bounding box) in the CSV file...\n",
        "                if self.include_classes == 'all' or int(row[self.input_format.index('class_id')].strip()) in self.include_classes: # If the class_id is among the classes that are to be included in the dataset...\n",
        "                    box = [] # Store the box class and coordinates here\n",
        "                    box.append(row[self.input_format.index('image_name')].strip()) # Select the image name column in the input format and append its content to `box`\n",
        "                    for element in self.box_output_format: # For each element in the output format (where the elements are the class ID and the four box coordinates)...\n",
        "                        box.append(int(row[self.input_format.index(element)].strip())) # ...select the respective column in the input format and append it to `box`.\n",
        "                    data.append(box)\n",
        "\n",
        "        data = sorted(data) # The data needs to be sorted, otherwise the next step won't give the correct result\n",
        "\n",
        "        # Now that we've made sure that the data is sorted by file names,\n",
        "        # we can compile the actual samples and labels lists\n",
        "\n",
        "        current_file = data[0][0] # The current image for which we're collecting the ground truth boxes\n",
        "        current_labels = [] # The list where we collect all ground truth boxes for a given image\n",
        "        add_to_dataset = False\n",
        "        for i, box in enumerate(data):\n",
        "\n",
        "            if box[0] == current_file: # If this box (i.e. this line of the CSV file) belongs to the current image file\n",
        "                current_labels.append(box[1:])\n",
        "                if i == len(data)-1: # If this is the last line of the CSV file\n",
        "                    if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                        p = np.random.uniform(0,1)\n",
        "                        if p >= (1-random_sample):\n",
        "                            self.labels.append(np.stack(current_labels, axis=0))\n",
        "                            self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                    else:\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "            else: # If this box belongs to a new image file\n",
        "                if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-random_sample):\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                else:\n",
        "                    self.labels.append(np.stack(current_labels, axis=0))\n",
        "                    self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                current_labels = [] # Reset the labels list because this is a new file.\n",
        "                current_file = box[0]\n",
        "                current_labels.append(box[1:])\n",
        "                if i == len(data)-1: # If this is the last line of the CSV file\n",
        "                    if random_sample: # In case we're not using the full dataset, but a random sample of it.\n",
        "                        p = np.random.uniform(0,1)\n",
        "                        if p >= (1-random_sample):\n",
        "                            self.labels.append(np.stack(current_labels, axis=0))\n",
        "                            self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "                    else:\n",
        "                        self.labels.append(np.stack(current_labels, axis=0))\n",
        "                        self.filenames.append(os.path.join(self.images_dir, current_file))\n",
        "\n",
        "        if ret: # In case we want to return these\n",
        "            return self.filenames, self.labels\n",
        "\n",
        "    def parse_xml(self,\n",
        "                  images_dirs,\n",
        "                  image_set_filenames,\n",
        "                  annotations_dirs=[],\n",
        "                  classes=['background',\n",
        "                           'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "                           'bottle', 'bus', 'car', 'cat',\n",
        "                           'chair', 'cow', 'diningtable', 'dog',\n",
        "                           'horse', 'motorbike', 'person', 'pottedplant',\n",
        "                           'sheep', 'sofa', 'train', 'tvmonitor'],\n",
        "                  include_classes = 'all',\n",
        "                  exclude_truncated=False,\n",
        "                  exclude_difficult=False,\n",
        "                  ret=False):\n",
        "        \n",
        "        # Set class members.\n",
        "        self.images_dirs = images_dirs\n",
        "        self.annotations_dirs = annotations_dirs\n",
        "        self.image_set_filenames = image_set_filenames\n",
        "        self.classes = classes\n",
        "        self.include_classes = include_classes\n",
        "\n",
        "        # Erase data that might have been parsed before.\n",
        "        self.filenames = []\n",
        "        self.image_ids = []\n",
        "        self.labels = []\n",
        "        if not annotations_dirs:\n",
        "            self.labels = None\n",
        "            annotations_dirs = [None] * len(images_dirs)\n",
        "\n",
        "        for images_dir, image_set_filename, annotations_dir in zip(images_dirs, image_set_filenames, annotations_dirs):\n",
        "            # Read the image set file that so that we know all the IDs of all the images to be included in the dataset.\n",
        "            with open(image_set_filename) as f:\n",
        "                image_ids = [line.strip() for line in f] # Note: These are strings, not integers.\n",
        "                self.image_ids += image_ids\n",
        "\n",
        "            # Loop over all images in this dataset.\n",
        "            for image_id in image_ids:\n",
        "\n",
        "                filename = '{}'.format(image_id) + '.jpg'\n",
        "                self.filenames.append(os.path.join(images_dir, filename))\n",
        "\n",
        "                if not annotations_dir is None:\n",
        "                    # Parse the XML file for this image.\n",
        "                    with open(os.path.join(annotations_dir, image_id + '.xml')) as f:\n",
        "                        soup = BeautifulSoup(f, 'xml')\n",
        "\n",
        "                    folder = soup.folder.text # In case we want to return the folder in addition to the image file name. Relevant for determining which dataset an image belongs to.\n",
        "                    #filename = soup.filename.text\n",
        "\n",
        "                    boxes = [] # We'll store all boxes for this image here\n",
        "                    objects = soup.find_all('object') # Get a list of all objects in this image\n",
        "\n",
        "                    # Parse the data for each object\n",
        "                    for obj in objects:\n",
        "                        class_name = obj.find('name').text\n",
        "                        class_id = self.classes.index(class_name)\n",
        "                        # Check if this class is supposed to be included in the dataset\n",
        "                        if (not self.include_classes == 'all') and (not class_id in self.include_classes): continue\n",
        "                        pose = obj.pose.text\n",
        "                        truncated = int(obj.truncated.text)\n",
        "                        if exclude_truncated and (truncated == 1): continue\n",
        "                        difficult = int(obj.difficult.text)\n",
        "                        if exclude_difficult and (difficult == 1): continue\n",
        "                        xmin = int(obj.bndbox.xmin.text)\n",
        "                        ymin = int(obj.bndbox.ymin.text)\n",
        "                        xmax = int(obj.bndbox.xmax.text)\n",
        "                        ymax = int(obj.bndbox.ymax.text)\n",
        "                        item_dict = {'folder': folder,\n",
        "                                     'image_name': filename,\n",
        "                                     'image_id': image_id,\n",
        "                                     'class_name': class_name,\n",
        "                                     'class_id': class_id,\n",
        "                                     'pose': pose,\n",
        "                                     'truncated': truncated,\n",
        "                                     'difficult': difficult,\n",
        "                                     'xmin': xmin,\n",
        "                                     'ymin': ymin,\n",
        "                                     'xmax': xmax,\n",
        "                                     'ymax': ymax}\n",
        "                        box = []\n",
        "                        for item in self.box_output_format:\n",
        "                            box.append(item_dict[item])\n",
        "                        boxes.append(box)\n",
        "\n",
        "                    self.labels.append(boxes)\n",
        "\n",
        "        if ret:\n",
        "            return self.filenames, self.labels, self.image_ids\n",
        "\n",
        "    def parse_json(self,\n",
        "                   images_dirs,\n",
        "                   annotations_filenames,\n",
        "                   ground_truth_available=False,\n",
        "                   include_classes = 'all',\n",
        "                   ret=False):\n",
        "     \n",
        "        self.images_dirs = images_dirs\n",
        "        self.annotations_filenames = annotations_filenames\n",
        "        self.include_classes = include_classes\n",
        "        # Erase data that might have been parsed before.\n",
        "        self.filenames = []\n",
        "        self.image_ids = []\n",
        "        self.labels = []\n",
        "        if not ground_truth_available:\n",
        "            self.labels = None\n",
        "\n",
        "        # Build the dictionaries that map between class names and class IDs.\n",
        "        with open(annotations_filenames[0], 'r') as f:\n",
        "            annotations = json.load(f)\n",
        "        \n",
        "        self.cats_to_names = {} # The map between class names (values) and their original IDs (keys)\n",
        "        self.classes_to_names = [] # A list of the class names with their indices representing the transformed IDs\n",
        "        self.classes_to_names.append('background') # Need to add the background class first so that the indexing is right.\n",
        "        self.cats_to_classes = {} # A dictionary that maps between the original (keys) and the transformed IDs (values)\n",
        "        self.classes_to_cats = {} # A dictionary that maps between the transformed (keys) and the original IDs (values)\n",
        "        for i, cat in enumerate(annotations['categories']):\n",
        "            self.cats_to_names[cat['id']] = cat['name']\n",
        "            self.classes_to_names.append(cat['name'])\n",
        "            self.cats_to_classes[cat['id']] = i + 1\n",
        "            self.classes_to_cats[i + 1] = cat['id']\n",
        "\n",
        "        # Iterate over all datasets.\n",
        "        for images_dir, annotations_filename in zip(self.images_dirs, self.annotations_filenames):\n",
        "            # Load the JSON file.\n",
        "            with open(annotations_filename, 'r') as f:\n",
        "                annotations = json.load(f)\n",
        "\n",
        "            if ground_truth_available:\n",
        "                # Create the annotations map, a dictionary whose keys are the image IDs\n",
        "                # and whose values are the annotations for the respective image ID.\n",
        "                image_ids_to_annotations = defaultdict(list)\n",
        "                for annotation in annotations['annotations']:\n",
        "                    image_ids_to_annotations[annotation['image_id']].append(annotation)\n",
        "\n",
        "            # Iterate over all images in the dataset.\n",
        "            for img in annotations['images']:\n",
        "\n",
        "                self.filenames.append(os.path.join(images_dir, img['file_name']))\n",
        "                self.image_ids.append(img['id'])\n",
        "\n",
        "                if ground_truth_available:\n",
        "                    # Get all annotations for this image.\n",
        "                    annotations = image_ids_to_annotations[img['id']]\n",
        "                    boxes = []\n",
        "                    for annotation in annotations:\n",
        "                        cat_id = annotation['category_id']\n",
        "                        # Check if this class is supposed to be included in the dataset.\n",
        "                        if (not self.include_classes == 'all') and (not cat_id in self.include_classes): continue\n",
        "                        # Transform the original class ID to fit in the sequence of consecutive IDs.\n",
        "                        class_id = self.cats_to_classes[cat_id]\n",
        "                        xmin = annotation['bbox'][0]\n",
        "                        ymin = annotation['bbox'][1]\n",
        "                        width = annotation['bbox'][2]\n",
        "                        height = annotation['bbox'][3]\n",
        "                        # Compute `xmax` and `ymax`.\n",
        "                        xmax = xmin + width\n",
        "                        ymax = ymin + height\n",
        "                        item_dict = {'image_name': img['file_name'],\n",
        "                                     'image_id': img['id'],\n",
        "                                     'class_id': class_id,\n",
        "                                     'xmin': xmin,\n",
        "                                     'ymin': ymin,\n",
        "                                     'xmax': xmax,\n",
        "                                     'ymax': ymax}\n",
        "                        box = []\n",
        "                        for item in self.box_output_format:\n",
        "                            box.append(item_dict[item])\n",
        "                        boxes.append(box)\n",
        "                    self.labels.append(boxes)\n",
        "\n",
        "        if ret:\n",
        "            return self.filenames, self.labels, self.image_ids\n",
        "\n",
        "    def save_filenames_and_labels(self, filenames_path='filenames.pkl', labels_path=None, image_ids_path=None):\n",
        "       \n",
        "        with open(filenames_path, 'wb') as f:\n",
        "            pickle.dump(self.filenames, f)\n",
        "        if not labels_path is None:\n",
        "            with open(labels_path, 'wb') as f:\n",
        "                pickle.dump(self.labels, f)\n",
        "        if not image_ids_path is None:\n",
        "            with open(image_ids_path, 'wb') as f:\n",
        "                pickle.dump(self.image_ids, f)\n",
        "\n",
        "    def generate(self,\n",
        "                 batch_size=32,\n",
        "                 shuffle=True,\n",
        "                 train=True,\n",
        "                 ssd_box_encoder=None,\n",
        "                 returns={'processed_images', 'encoded_labels'},\n",
        "                 convert_to_3_channels=True,\n",
        "                 equalize=False,\n",
        "                 brightness=False,\n",
        "                 flip=False,\n",
        "                 translate=False,\n",
        "                 scale=False,\n",
        "                 max_crop_and_resize=False,\n",
        "                 random_pad_and_resize=False,\n",
        "                 random_crop=False,\n",
        "                 crop=False,\n",
        "                 resize=False,\n",
        "                 gray=False,\n",
        "                 limit_boxes=True,\n",
        "                 include_thresh=0.3,\n",
        "                 subtract_mean=None,\n",
        "                 divide_by_stddev=None,\n",
        "                 swap_channels=False,\n",
        "                 keep_images_without_gt=False):\n",
        "    \n",
        "        if shuffle: # Shuffle the data before we begin\n",
        "            if (self.labels is None) and (self.image_ids is None):\n",
        "                self.filenames = sklearn.utils.shuffle(self.filenames)\n",
        "            elif (self.labels is None):\n",
        "                self.filenames, self.image_ids = sklearn.utils.shuffle(self.filenames, self.image_ids)\n",
        "            elif (self.image_ids is None):\n",
        "                self.filenames, self.labels = sklearn.utils.shuffle(self.filenames, self.labels)\n",
        "            else:\n",
        "                self.filenames, self.labels, self.image_ids = sklearn.utils.shuffle(self.filenames, self.labels, self.image_ids)\n",
        "        current = 0\n",
        "\n",
        "        # Find out the indices of the box coordinates in the label data\n",
        "        xmin = self.box_output_format.index('xmin')\n",
        "        ymin = self.box_output_format.index('ymin')\n",
        "        xmax = self.box_output_format.index('xmax')\n",
        "        ymax = self.box_output_format.index('ymax')\n",
        "        ios = np.amin([xmin, ymin, xmax, ymax]) # Index offset, we need this for the inverse coordinate transform indices.\n",
        "\n",
        "        while True:\n",
        "\n",
        "            batch_X, batch_y = [], []\n",
        "\n",
        "            if current >= len(self.filenames):\n",
        "                current = 0\n",
        "                if shuffle:\n",
        "                    # Shuffle the data after each complete pass\n",
        "                    if (self.labels is None) and (self.image_ids is None):\n",
        "                        self.filenames = sklearn.utils.shuffle(self.filenames)\n",
        "                    elif (self.labels is None):\n",
        "                        self.filenames, self.image_ids = sklearn.utils.shuffle(self.filenames, self.image_ids)\n",
        "                    elif (self.image_ids is None):\n",
        "                        self.filenames, self.labels = sklearn.utils.shuffle(self.filenames, self.labels)\n",
        "                    else:\n",
        "                        self.filenames, self.labels, self.image_ids = sklearn.utils.shuffle(self.filenames, self.labels, self.image_ids)\n",
        "\n",
        "            # Get the image filepaths for this batch.\n",
        "            batch_filenames = self.filenames[current:current+batch_size]\n",
        "\n",
        "            # Load the images for this batch.\n",
        "            for filename in batch_filenames:\n",
        "                with Image.open(filename) as img:\n",
        "                    batch_X.append(np.array(img))\n",
        "\n",
        "            # Get the labels for this batch (if there are any).\n",
        "            if not (self.labels is None):\n",
        "                batch_y = deepcopy(self.labels[current:current+batch_size])\n",
        "            else:\n",
        "                batch_y = None\n",
        "\n",
        "            # Get the image IDs for this batch (if there are any).\n",
        "            if not self.image_ids is None:\n",
        "                batch_image_ids = self.image_ids[current:current+batch_size]\n",
        "            else:\n",
        "                batch_image_ids = None\n",
        "\n",
        "            # Create the array that is to contain the inverse coordinate transformation values for this batch.\n",
        "            batch_inverse_coord_transform = np.array([[[0, 1]] * 4] * batch_size, dtype=np.float) # Array of shape `(batch_size, 4, 2)`, where the last axis contains an additive and a multiplicative scalar transformation constant.\n",
        "\n",
        "            if 'original_images' in returns:\n",
        "                batch_original_images = deepcopy(batch_X) # The original, unaltered images\n",
        "            if 'original_labels' in returns and not batch_y is None:\n",
        "                batch_original_labels = deepcopy(batch_y) # The original, unaltered labels\n",
        "\n",
        "            current += batch_size\n",
        "\n",
        "            batch_items_to_remove = [] # In case we need to remove any images from the batch because of failed random cropping, store their indices in this list.\n",
        "\n",
        "            for i in range(len(batch_X)):\n",
        "\n",
        "                img_height, img_width = batch_X[i].shape[0], batch_X[i].shape[1]\n",
        "\n",
        "                if not batch_y is None:\n",
        "                    # If this image has no ground truth boxes, maybe we don't want to keep it in the batch.\n",
        "                    if (len(batch_y[i]) == 0) and not keep_images_without_gt:\n",
        "                        batch_items_to_remove.append(i)\n",
        "                    # Convert labels into an array (in case it isn't one already), otherwise the indexing below breaks.\n",
        "                    batch_y[i] = np.array(batch_y[i])\n",
        "\n",
        "                # From here on, perform some optional image transformations.\n",
        "\n",
        "                if (batch_X[i].ndim == 2) and convert_to_3_channels:\n",
        "                    # Convert the 1-channel image into a 3-channel image.\n",
        "                    batch_X[i] = np.stack([batch_X[i]] * 3, axis=-1)\n",
        "\n",
        "                if equalize:\n",
        "                    batch_X[i] = histogram_eq(batch_X[i])\n",
        "\n",
        "                if brightness:\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-brightness[2]):\n",
        "                        batch_X[i] = _brightness(batch_X[i], min=brightness[0], max=brightness[1])\n",
        "\n",
        "                if flip: # Performs flips along the vertical axis only (i.e. horizontal flips).\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-flip):\n",
        "                        batch_X[i] = _flip(batch_X[i])\n",
        "                        if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                            batch_y[i][:,[xmin,xmax]] = img_width - batch_y[i][:,[xmax,xmin]] # xmin and xmax are swapped when mirrored\n",
        "\n",
        "                if translate:\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-translate[2]):\n",
        "                        # Translate the image and return the shift values so that we can adjust the labels\n",
        "                        batch_X[i], xshift, yshift = _translate(batch_X[i], translate[0], translate[1])\n",
        "                        if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                            # Adjust the box coordinates.\n",
        "                            batch_y[i][:,[xmin,xmax]] += xshift\n",
        "                            batch_y[i][:,[ymin,ymax]] += yshift\n",
        "                            # Limit the box coordinates to lie within the image boundaries\n",
        "                            if limit_boxes:\n",
        "                                before_limiting = deepcopy(batch_y[i])\n",
        "                                x_coords = batch_y[i][:,[xmin,xmax]]\n",
        "                                x_coords[x_coords >= img_width] = img_width - 1\n",
        "                                x_coords[x_coords < 0] = 0\n",
        "                                batch_y[i][:,[xmin,xmax]] = x_coords\n",
        "                                y_coords = batch_y[i][:,[ymin,ymax]]\n",
        "                                y_coords[y_coords >= img_height] = img_height - 1\n",
        "                                y_coords[y_coords < 0] = 0\n",
        "                                batch_y[i][:,[ymin,ymax]] = y_coords\n",
        "                                \n",
        "                                before_area = (before_limiting[:,xmax] - before_limiting[:,xmin]) * (before_limiting[:,ymax] - before_limiting[:,ymin])\n",
        "                                after_area = (batch_y[i][:,xmax] - batch_y[i][:,xmin]) * (batch_y[i][:,ymax] - batch_y[i][:,ymin])\n",
        "                                if include_thresh == 0: batch_y[i] = batch_y[i][after_area > include_thresh * before_area] # If `include_thresh == 0`, we want to make sure that boxes with area 0 get thrown out, hence the \">\" sign instead of the \">=\" sign\n",
        "                                else: batch_y[i] = batch_y[i][after_area >= include_thresh * before_area] # Especially for the case `include_thresh == 1` we want the \">=\" sign, otherwise no boxes would be left at all\n",
        "\n",
        "                if scale:\n",
        "                    p = np.random.uniform(0,1)\n",
        "                    if p >= (1-scale[2]):\n",
        "                        # Rescale the image and return the transformation matrix M so we can use it to adjust the box coordinates\n",
        "                        batch_X[i], M, scale_factor = _scale(batch_X[i], scale[0], scale[1])\n",
        "                        if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                            # Adjust the box coordinates.\n",
        "                            # Transform two opposite corner points of the rectangular boxes using the transformation matrix `M`\n",
        "                            toplefts = np.array([batch_y[i][:,xmin], batch_y[i][:,ymin], np.ones(batch_y[i].shape[0])])\n",
        "                            bottomrights = np.array([batch_y[i][:,xmax], batch_y[i][:,ymax], np.ones(batch_y[i].shape[0])])\n",
        "                            new_toplefts = (np.dot(M, toplefts)).T\n",
        "                            new_bottomrights = (np.dot(M, bottomrights)).T\n",
        "                            batch_y[i][:,[xmin,ymin]] = new_toplefts.astype(np.int)\n",
        "                            batch_y[i][:,[xmax,ymax]] = new_bottomrights.astype(np.int)\n",
        "                            # Limit the box coordinates to lie within the image boundaries\n",
        "                            if limit_boxes and (scale_factor > 1): # We don't need to do any limiting in case we shrunk the image\n",
        "                                before_limiting = deepcopy(batch_y[i])\n",
        "                                x_coords = batch_y[i][:,[xmin,xmax]]\n",
        "                                x_coords[x_coords >= img_width] = img_width - 1\n",
        "                                x_coords[x_coords < 0] = 0\n",
        "                                batch_y[i][:,[xmin,xmax]] = x_coords\n",
        "                                y_coords = batch_y[i][:,[ymin,ymax]]\n",
        "                                y_coords[y_coords >= img_height] = img_height - 1\n",
        "                                y_coords[y_coords < 0] = 0\n",
        "                                batch_y[i][:,[ymin,ymax]] = y_coords\n",
        "                                \n",
        "                                before_area = (before_limiting[:,xmax] - before_limiting[:,xmin]) * (before_limiting[:,ymax] - before_limiting[:,ymin])\n",
        "                                after_area = (batch_y[i][:,xmax] - batch_y[i][:,xmin]) * (batch_y[i][:,ymax] - batch_y[i][:,ymin])\n",
        "                                if include_thresh == 0: batch_y[i] = batch_y[i][after_area > include_thresh * before_area] # If `include_thresh == 0`, we want to make sure that boxes with area 0 get thrown out, hence the \">\" sign instead of the \">=\" sign\n",
        "                                else: batch_y[i] = batch_y[i][after_area >= include_thresh * before_area] # Especially for the case `include_thresh == 1` we want the \">=\" sign, otherwise no boxes would be left at all\n",
        "\n",
        "                if max_crop_and_resize:\n",
        "                    # The ratio of the two aspect ratios (source image and target size) determines the maximal possible crop.\n",
        "                    image_aspect_ratio = img_width / img_height\n",
        "                    resize_aspect_ratio = max_crop_and_resize[1] / max_crop_and_resize[0]\n",
        "\n",
        "                    if image_aspect_ratio < resize_aspect_ratio:\n",
        "                        crop_width = img_width\n",
        "                        crop_height = int(round(crop_width / resize_aspect_ratio))\n",
        "                    else:\n",
        "                        crop_height = img_height\n",
        "                        crop_width = int(round(crop_height * resize_aspect_ratio))\n",
        "                    # The actual cropping and resizing will be done by the random crop and resizing operations below.\n",
        "                    # Here, we only set the parameters for them.\n",
        "                    random_crop = (crop_height, crop_width, max_crop_and_resize[2], max_crop_and_resize[3])\n",
        "                    resize = (max_crop_and_resize[0], max_crop_and_resize[1])\n",
        "\n",
        "                if random_pad_and_resize:\n",
        "\n",
        "                    resize_aspect_ratio = random_pad_and_resize[1] / random_pad_and_resize[0]\n",
        "\n",
        "                    if img_width < img_height:\n",
        "                        crop_height = img_height\n",
        "                        crop_width = int(round(crop_height * resize_aspect_ratio))\n",
        "                    else:\n",
        "                        crop_width = img_width\n",
        "                        crop_height = int(round(crop_width / resize_aspect_ratio))\n",
        "                    # The actual cropping and resizing will be done by the random crop and resizing operations below.\n",
        "                    # Here, we only set the parameters for them.\n",
        "                    if max_crop_and_resize:\n",
        "                        p = np.random.uniform(0,1)\n",
        "                        if p >= (1-random_pad_and_resize[4]):\n",
        "                            random_crop = (crop_height, crop_width, random_pad_and_resize[2], random_pad_and_resize[3])\n",
        "                            resize = (random_pad_and_resize[0], random_pad_and_resize[1])\n",
        "                    else:\n",
        "                        random_crop = (crop_height, crop_width, random_pad_and_resize[2], random_pad_and_resize[3])\n",
        "                        resize = (random_pad_and_resize[0], random_pad_and_resize[1])\n",
        "\n",
        "                if random_crop:\n",
        "                    \n",
        "                    y_range = img_height - random_crop[0]\n",
        "                    x_range = img_width - random_crop[1]\n",
        "                    # Keep track of the number of trials and of whether or not the most recent crop contains at least one object\n",
        "                    min_1_object_fulfilled = False\n",
        "                    trial_counter = 0\n",
        "                    while (not min_1_object_fulfilled) and (trial_counter < random_crop[3]):\n",
        "                        # Select a random crop position from the possible crop positions\n",
        "                        if y_range >= 0: crop_ymin = np.random.randint(0, y_range + 1) # There are y_range + 1 possible positions for the crop in the vertical dimension\n",
        "                        else: crop_ymin = np.random.randint(0, -y_range + 1) # The possible positions for the image on the background canvas in the vertical dimension\n",
        "                        if x_range >= 0: crop_xmin = np.random.randint(0, x_range + 1) # There are x_range + 1 possible positions for the crop in the horizontal dimension\n",
        "                        else: crop_xmin = np.random.randint(0, -x_range + 1) # The possible positions for the image on the background canvas in the horizontal dimension\n",
        "                        # Perform the crop\n",
        "                        if y_range >= 0 and x_range >= 0: # If the patch to be cropped out is smaller than the original image in both dimenstions, we just perform a regular crop\n",
        "                            # Crop the image\n",
        "                            patch_X = np.copy(batch_X[i][crop_ymin:crop_ymin+random_crop[0], crop_xmin:crop_xmin+random_crop[1]])\n",
        "                            # Add the parameters to reverse this transformation.\n",
        "                            patch_y_inverse_y = crop_ymin\n",
        "                            patch_y_inverse_x = crop_xmin\n",
        "                            if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                                # Translate the box coordinates into the new coordinate system: Cropping shifts the origin by `(crop_ymin, crop_xmin)`\n",
        "                                patch_y = np.copy(batch_y[i])\n",
        "                                patch_y[:,[ymin,ymax]] -= crop_ymin\n",
        "                                patch_y[:,[xmin,xmax]] -= crop_xmin\n",
        "                                # Limit the box coordinates to lie within the new image boundaries\n",
        "                                if limit_boxes:\n",
        "                                    # Both the x- and y-coordinates might need to be limited\n",
        "                                    before_limiting = np.copy(patch_y)\n",
        "                                    y_coords = patch_y[:,[ymin,ymax]]\n",
        "                                    y_coords[y_coords < 0] = 0\n",
        "                                    y_coords[y_coords >= random_crop[0]] = random_crop[0] - 1\n",
        "                                    patch_y[:,[ymin,ymax]] = y_coords\n",
        "                                    x_coords = patch_y[:,[xmin,xmax]]\n",
        "                                    x_coords[x_coords < 0] = 0\n",
        "                                    x_coords[x_coords >= random_crop[1]] = random_crop[1] - 1\n",
        "                                    patch_y[:,[xmin,xmax]] = x_coords\n",
        "                        elif y_range >= 0 and x_range < 0: # If the crop is larger than the original image in the horizontal dimension only,...\n",
        "                            # Crop the image\n",
        "                            patch_X = np.copy(batch_X[i][crop_ymin:crop_ymin+random_crop[0]]) # ...crop the vertical dimension just as before,...\n",
        "                            canvas = np.zeros((random_crop[0], random_crop[1], patch_X.shape[2]), dtype=np.uint8) # ...generate a blank background image to place the patch onto,...\n",
        "                            canvas[:, crop_xmin:crop_xmin+img_width] = patch_X # ...and place the patch onto the canvas at the random `crop_xmin` position computed above.\n",
        "                            patch_X = canvas\n",
        "                            # Add the parameters to reverse this transformation.\n",
        "                            patch_y_inverse_y = crop_ymin\n",
        "                            patch_y_inverse_x = -crop_xmin\n",
        "                            if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                                # Translate the box coordinates into the new coordinate system: In this case, the origin is shifted by `(crop_ymin, -crop_xmin)`\n",
        "                                patch_y = np.copy(batch_y[i])\n",
        "                                patch_y[:,[ymin,ymax]] -= crop_ymin\n",
        "                                patch_y[:,[xmin,xmax]] += crop_xmin\n",
        "                                # Limit the box coordinates to lie within the new image boundaries\n",
        "                                if limit_boxes:\n",
        "                                    # Only the y-coordinates might need to be limited\n",
        "                                    before_limiting = np.copy(patch_y)\n",
        "                                    y_coords = patch_y[:,[ymin,ymax]]\n",
        "                                    y_coords[y_coords < 0] = 0\n",
        "                                    y_coords[y_coords >= random_crop[0]] = random_crop[0] - 1\n",
        "                                    patch_y[:,[ymin,ymax]] = y_coords\n",
        "                        elif y_range < 0 and x_range >= 0: # If the crop is larger than the original image in the vertical dimension only,...\n",
        "                            # Crop the image\n",
        "                            patch_X = np.copy(batch_X[i][:,crop_xmin:crop_xmin+random_crop[1]]) # ...crop the horizontal dimension just as in the first case,...\n",
        "                            canvas = np.zeros((random_crop[0], random_crop[1], patch_X.shape[2]), dtype=np.uint8) # ...generate a blank background image to place the patch onto,...\n",
        "                            canvas[crop_ymin:crop_ymin+img_height, :] = patch_X # ...and place the patch onto the canvas at the random `crop_ymin` position computed above.\n",
        "                            patch_X = canvas\n",
        "                            # Add the parameters to reverse this transformation.\n",
        "                            patch_y_inverse_y = -crop_ymin\n",
        "                            patch_y_inverse_x = crop_xmin\n",
        "                            if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                                # Translate the box coordinates into the new coordinate system: In this case, the origin is shifted by `(-crop_ymin, crop_xmin)`\n",
        "                                patch_y = np.copy(batch_y[i])\n",
        "                                patch_y[:,[ymin,ymax]] += crop_ymin\n",
        "                                patch_y[:,[xmin,xmax]] -= crop_xmin\n",
        "                                # Limit the box coordinates to lie within the new image boundaries\n",
        "                                if limit_boxes:\n",
        "                                    # Only the x-coordinates might need to be limited\n",
        "                                    before_limiting = np.copy(patch_y)\n",
        "                                    x_coords = patch_y[:,[xmin,xmax]]\n",
        "                                    x_coords[x_coords < 0] = 0\n",
        "                                    x_coords[x_coords >= random_crop[1]] = random_crop[1] - 1\n",
        "                                    patch_y[:,[xmin,xmax]] = x_coords\n",
        "                        else:  # If the crop is larger than the original image in both dimensions,...\n",
        "                            patch_X = np.copy(batch_X[i])\n",
        "                            canvas = np.zeros((random_crop[0], random_crop[1], patch_X.shape[2]), dtype=np.uint8) # ...generate a blank background image to place the patch onto,...\n",
        "                            canvas[crop_ymin:crop_ymin+img_height, crop_xmin:crop_xmin+img_width] = patch_X # ...and place the patch onto the canvas at the random `(crop_ymin, crop_xmin)` position computed above.\n",
        "                            patch_X = canvas\n",
        "                            # Add the parameters to reverse this transformation.\n",
        "                            patch_y_inverse_y = -crop_ymin\n",
        "                            patch_y_inverse_x = -crop_xmin\n",
        "                            if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                                # Translate the box coordinates into the new coordinate system: In this case, the origin is shifted by `(-crop_ymin, -crop_xmin)`\n",
        "                                patch_y = np.copy(batch_y[i])\n",
        "                                patch_y[:,[ymin,ymax]] += crop_ymin\n",
        "                                patch_y[:,[xmin,xmax]] += crop_xmin\n",
        "                                # Note that no limiting is necessary in this case\n",
        "                        if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                            \n",
        "                            if limit_boxes and (y_range >= 0 or x_range >= 0):\n",
        "                                before_area = (before_limiting[:,xmax] - before_limiting[:,xmin]) * (before_limiting[:,ymax] - before_limiting[:,ymin])\n",
        "                                after_area = (patch_y[:,xmax] - patch_y[:,xmin]) * (patch_y[:,ymax] - patch_y[:,ymin])\n",
        "                                if include_thresh == 0: patch_y = patch_y[after_area > include_thresh * before_area] # If `include_thresh == 0`, we want to make sure that boxes with area 0 get thrown out, hence the \">\" sign instead of the \">=\" sign\n",
        "                                else: patch_y = patch_y[after_area >= include_thresh * before_area] # Especially for the case `include_thresh == 1` we want the \">=\" sign, otherwise no boxes would be left at all\n",
        "                            trial_counter += 1 # We've just used one of our trials\n",
        "                            # Check if we have found a valid crop\n",
        "                            if random_crop[2] == 0: # If `min_1_object == 0`, break out of the while loop after the first loop because we are fine with whatever crop we got\n",
        "                                batch_X[i] = patch_X # The cropped patch becomes our new batch item\n",
        "                                batch_y[i] = patch_y # The adjusted boxes become our new labels for this batch item\n",
        "                                batch_inverse_coord_transform[i,[ymin-ios,ymax-ios],0] += patch_y_inverse_y\n",
        "                                batch_inverse_coord_transform[i,[xmin-ios,xmax-ios],0] += patch_y_inverse_x\n",
        "                                break\n",
        "                            elif len(patch_y) > 0: # If we have at least one object left, this crop is valid and we can stop\n",
        "                                min_1_object_fulfilled = True\n",
        "                                batch_X[i] = patch_X # The cropped patch becomes our new batch item\n",
        "                                batch_y[i] = patch_y # The adjusted boxes become our new labels for this batch item\n",
        "                                batch_inverse_coord_transform[i,[ymin-ios,ymax-ios],0] += patch_y_inverse_y\n",
        "                                batch_inverse_coord_transform[i,[xmin-ios,xmax-ios],0] += patch_y_inverse_x\n",
        "                            elif (trial_counter >= random_crop[3]) and (not i in batch_items_to_remove): # If we've reached the trial limit and still not found a valid crop, remove this image from the batch\n",
        "                                batch_items_to_remove.append(i)\n",
        "                        else: # If `batch_y` is `None`, i.e. if we don't have ground truth data, any crop is a valid crop.\n",
        "                            batch_X[i] = patch_X # The cropped patch becomes our new batch item\n",
        "                            batch_inverse_coord_transform[i,[ymin-ios,ymax-ios],0] += patch_y_inverse_y\n",
        "                            batch_inverse_coord_transform[i,[xmin-ios,xmax-ios],0] += patch_y_inverse_x\n",
        "                            break\n",
        "                    # Update the image size so that subsequent transformations can work correctly.\n",
        "                    img_height = random_crop[0]\n",
        "                    img_width = random_crop[1]\n",
        "\n",
        "                if crop:\n",
        "                    # Crop the image\n",
        "                    batch_X[i] = np.copy(batch_X[i][crop[0]:img_height-crop[1], crop[2]:img_width-crop[3]])\n",
        "                    # Update the image size so that subsequent transformations can work correctly\n",
        "                    img_height -= crop[0] + crop[1]\n",
        "                    img_width -= crop[2] + crop[3]\n",
        "                    if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                        \n",
        "                        if crop[0] > 0:\n",
        "                            batch_y[i][:,[ymin,ymax]] -= crop[0]\n",
        "                        if crop[2] > 0:\n",
        "                            batch_y[i][:,[xmin,xmax]] -= crop[2]\n",
        "                        \n",
        "                        if limit_boxes:\n",
        "                            before_limiting = np.copy(batch_y[i])\n",
        "                            \n",
        "                            if crop[0] > 0:\n",
        "                                y_coords = batch_y[i][:,[ymin,ymax]]\n",
        "                                y_coords[y_coords < 0] = 0\n",
        "                                batch_y[i][:,[ymin,ymax]] = y_coords\n",
        "                            if crop[1] > 0:\n",
        "                                y_coords = batch_y[i][:,[ymin,ymax]]\n",
        "                                y_coords[y_coords >= img_height] = img_height - 1\n",
        "                                batch_y[i][:,[ymin,ymax]] = y_coords\n",
        "                            if crop[2] > 0:\n",
        "                                x_coords = batch_y[i][:,[xmin,xmax]]\n",
        "                                x_coords[x_coords < 0] = 0\n",
        "                                batch_y[i][:,[xmin,xmax]] = x_coords\n",
        "                            if crop[3] > 0:\n",
        "                                x_coords = batch_y[i][:,[xmin,xmax]]\n",
        "                                x_coords[x_coords >= img_width] = img_width - 1\n",
        "                                batch_y[i][:,[xmin,xmax]] = x_coords\n",
        "                           \n",
        "                            before_area = (before_limiting[:,xmax] - before_limiting[:,xmin]) * (before_limiting[:,ymax] - before_limiting[:,ymin])\n",
        "                            after_area = (batch_y[i][:,xmax] - batch_y[i][:,xmin]) * (batch_y[i][:,ymax] - batch_y[i][:,ymin])\n",
        "                            if include_thresh == 0: batch_y[i] = batch_y[i][after_area > include_thresh * before_area] # If `include_thresh == 0`, we want to make sure that boxes with area 0 get thrown out, hence the \">\" sign instead of the \">=\" sign\n",
        "                            else: batch_y[i] = batch_y[i][after_area >= include_thresh * before_area] # Especially for the case `include_thresh == 1` we want the \">=\" sign, otherwise no boxes would be left at all\n",
        "\n",
        "                if resize:\n",
        "                    batch_X[i] = cv2.resize(batch_X[i], dsize=(resize[1], resize[0]))\n",
        "                    batch_inverse_coord_transform[i,[ymin-ios,ymax-ios],1] *= (img_height / resize[0])\n",
        "                    batch_inverse_coord_transform[i,[xmin-ios,xmax-ios],1] *= (img_width / resize[1])\n",
        "                    if not ((batch_y is None) or (len(batch_y[i]) == 0)):\n",
        "                        batch_y[i][:,[ymin,ymax]] = batch_y[i][:,[ymin,ymax]] * (resize[0] / img_height)\n",
        "                        batch_y[i][:,[xmin,xmax]] = batch_y[i][:,[xmin,xmax]] * (resize[1] / img_width)\n",
        "                    img_width, img_height = resize # Updating these at this point is unnecessary, but it's one fewer source of error if this method gets expanded in the future.\n",
        "\n",
        "                if gray:\n",
        "                    batch_X[i] = cv2.cvtColor(batch_X[i], cv2.COLOR_RGB2GRAY)\n",
        "                    if convert_to_3_channels:\n",
        "                        batch_X[i] = np.stack([batch_X[i]] * 3, axis=-1)\n",
        "                    else:\n",
        "                        batch_X[i] = np.expand_dims(batch_X[i], axis=-1)\n",
        "\n",
        "           \n",
        "            batch_X = np.array(batch_X)\n",
        "\n",
        "            if not keep_images_without_gt:\n",
        "                # If any batch items need to be removed because of failed random cropping, remove them now.\n",
        "                batch_inverse_coord_transform = np.delete(batch_inverse_coord_transform, batch_items_to_remove, axis=0)\n",
        "                batch_X = np.delete(batch_X, batch_items_to_remove, axis=0)\n",
        "                for j in sorted(batch_items_to_remove, reverse=True):\n",
        "                    # This isn't efficient, but it hopefully should not need to be done often anyway.\n",
        "                    batch_filenames.pop(j)\n",
        "                    if not batch_y is None: batch_y.pop(j)\n",
        "                    if not batch_imgage_ids is None: batch_image_ids.pop(j)\n",
        "                    if 'original_images' in returns: batch_original_images.pop(j)\n",
        "                    if 'original_labels' in returns and not batch_y is None: batch_original_labels.pop(j)\n",
        "\n",
        "            # Perform image transformations that can be bulk-applied to the whole batch.\n",
        "            if not (subtract_mean is None):\n",
        "                batch_X = batch_X.astype(np.int16) - np.array(subtract_mean)\n",
        "            if not (divide_by_stddev is None):\n",
        "                batch_X = batch_X.astype(np.int16) / np.array(divide_by_stddev)\n",
        "            if swap_channels:\n",
        "                batch_X = batch_X[:,:,:,[2, 1, 0]]\n",
        "\n",
        "            if train: # During training we need the encoded labels instead of the format that `batch_y` has\n",
        "                if ssd_box_encoder is None:\n",
        "                    raise ValueError(\"`ssd_box_encoder` cannot be `None` in training mode.\")\n",
        "                if 'matched_anchors' in returns:\n",
        "                    batch_y_true, batch_matched_anchors = ssd_box_encoder.encode_y(batch_y, diagnostics=True) # Encode the labels into the `y_true` tensor that the SSD loss function needs.\n",
        "                else:\n",
        "                    batch_y_true = ssd_box_encoder.encode_y(batch_y, diagnostics=False) # Encode the labels into the `y_true` tensor that the SSD loss function needs.\n",
        "\n",
        "            # Compile the output.\n",
        "            ret = []\n",
        "            ret.append(batch_X)\n",
        "            if train:\n",
        "                ret.append(batch_y_true)\n",
        "                if 'matched_anchors' in returns: ret.append(batch_matched_anchors)\n",
        "            if 'processed_labels' in returns and not batch_y is None: ret.append(batch_y)\n",
        "            if 'filenames' in returns: ret.append(batch_filenames)\n",
        "            if 'image_ids' in returns and not batch_image_ids is None: ret.append(batch_image_ids)\n",
        "            if 'inverse_transform' in returns: ret.append(batch_inverse_coord_transform)\n",
        "            if 'original_images' in returns: ret.append(batch_original_images)\n",
        "            if 'original_labels' in returns and not batch_y is None: ret.append(batch_original_labels)\n",
        "\n",
        "            yield ret\n",
        "\n",
        "    def get_filenames_labels(self):\n",
        "        \n",
        "        return self.filenames, self.labels, self.image_ids\n",
        "\n",
        "    def get_n_samples(self):\n",
        "        \n",
        "        return len(self.filenames)\n",
        "\n",
        "    def process_offline(self,\n",
        "                        dest_path='',\n",
        "                        start=0,\n",
        "                        stop='all',\n",
        "                        crop=False,\n",
        "                        equalize=False,\n",
        "                        brightness=False,\n",
        "                        flip=False,\n",
        "                        translate=False,\n",
        "                        scale=False,\n",
        "                        resize=False,\n",
        "                        gray=False,\n",
        "                        limit_boxes=True,\n",
        "                        include_thresh=0.3,\n",
        "                        diagnostics=False):\n",
        "        \n",
        "        import gc\n",
        "\n",
        "        targets_for_csv = []\n",
        "        if stop == 'all':\n",
        "            stop = len(self.filenames)\n",
        "\n",
        "        if diagnostics:\n",
        "            processed_images = []\n",
        "            original_images = []\n",
        "            processed_labels = []\n",
        "\n",
        "        # Find out the indices of the box coordinates in the label data\n",
        "        xmin = self.box_output_format.index('xmin')\n",
        "        xmax = self.box_output_format.index('xmax')\n",
        "        ymin = self.box_output_format.index('ymin')\n",
        "        ymax = self.box_output_format.index('ymax')\n",
        "\n",
        "        for k, filename in enumerate(self.filenames[start:stop]):\n",
        "            i = k + start\n",
        "            with Image.open('{}'.format(os.path.join(self.images_path, filename))) as img:\n",
        "                image = np.array(img)\n",
        "            targets = np.copy(self.labels[i])\n",
        "\n",
        "            if diagnostics:\n",
        "                original_images.append(image)\n",
        "\n",
        "            img_height, img_width, ch = image.shape\n",
        "\n",
        "            if equalize:\n",
        "                image = histogram_eq(image)\n",
        "\n",
        "            if brightness:\n",
        "                p = np.random.uniform(0,1)\n",
        "                if p >= (1-brightness[2]):\n",
        "                    image = _brightness(image, min=brightness[0], max=brightness[1])\n",
        "\n",
        "            \n",
        "            if flip:\n",
        "                p = np.random.uniform(0,1)\n",
        "                if p >= (1-flip):\n",
        "                    image = _flip(image)\n",
        "                    targets[:,[0,1]] = img_width - targets[:,[1,0]] # xmin and xmax are swapped when mirrored\n",
        "\n",
        "            if translate:\n",
        "                p = np.random.uniform(0,1)\n",
        "                if p >= (1-translate[2]):\n",
        "                    image, xshift, yshift = _translate(image, translate[0], translate[1])\n",
        "                    targets[:,[0,1]] += xshift\n",
        "                    targets[:,[2,3]] += yshift\n",
        "                    if limit_boxes:\n",
        "                        before_limiting = np.copy(targets)\n",
        "                        x_coords = targets[:,[0,1]]\n",
        "                        x_coords[x_coords >= img_width] = img_width - 1\n",
        "                        x_coords[x_coords < 0] = 0\n",
        "                        targets[:,[0,1]] = x_coords\n",
        "                        y_coords = targets[:,[2,3]]\n",
        "                        y_coords[y_coords >= img_height] = img_height - 1\n",
        "                        y_coords[y_coords < 0] = 0\n",
        "                        targets[:,[2,3]] = y_coords\n",
        "                        \n",
        "                        before_area = (before_limiting[:,1] - before_limiting[:,0]) * (before_limiting[:,3] - before_limiting[:,2])\n",
        "                        after_area = (targets[:,1] - targets[:,0]) * (targets[:,3] - targets[:,2])\n",
        "                        targets = targets[after_area >= include_thresh * before_area]\n",
        "\n",
        "            if scale:\n",
        "                p = np.random.uniform(0,1)\n",
        "                if p >= (1-scale[2]):\n",
        "                    image, M, scale_factor = _scale(image, scale[0], scale[1])\n",
        "                    \n",
        "                    toplefts = np.array([targets[:,0], targets[:,2], np.ones(targets.shape[0])])\n",
        "                    bottomrights = np.array([targets[:,1], targets[:,3], np.ones(targets.shape[0])])\n",
        "                    new_toplefts = (np.dot(M, toplefts)).T\n",
        "                    new_bottomrights = (np.dot(M, bottomrights)).T\n",
        "                    targets[:,[0,2]] = new_toplefts.astype(np.int)\n",
        "                    targets[:,[1,3]] = new_bottomrights.astype(np.int)\n",
        "                    if limit_boxes and (scale_factor > 1): # We don't need to do any limiting in case we shrunk the image\n",
        "                        before_limiting = np.copy(targets)\n",
        "                        x_coords = targets[:,[0,1]]\n",
        "                        x_coords[x_coords >= img_width] = img_width - 1\n",
        "                        x_coords[x_coords < 0] = 0\n",
        "                        targets[:,[0,1]] = x_coords\n",
        "                        y_coords = targets[:,[2,3]]\n",
        "                        y_coords[y_coords >= img_height] = img_height - 1\n",
        "                        y_coords[y_coords < 0] = 0\n",
        "                        targets[:,[2,3]] = y_coords\n",
        "                        \n",
        "                        before_area = (before_limiting[:,1] - before_limiting[:,0]) * (before_limiting[:,3] - before_limiting[:,2])\n",
        "                        after_area = (targets[:,1] - targets[:,0]) * (targets[:,3] - targets[:,2])\n",
        "                        targets = targets[after_area >= include_thresh * before_area]\n",
        "\n",
        "            if crop:\n",
        "                image = image[crop[0]:img_height-crop[1], crop[2]:img_width-crop[3]]\n",
        "                if limit_boxes: # Adjust boxes affected by cropping and remove those that will no longer be in the image\n",
        "                    before_limiting = np.copy(targets)\n",
        "                    if crop[0] > 0:\n",
        "                        y_coords = targets[:,[2,3]]\n",
        "                        y_coords[y_coords < crop[0]] = crop[0]\n",
        "                        targets[:,[2,3]] = y_coords\n",
        "                    if crop[1] > 0:\n",
        "                        y_coords = targets[:,[2,3]]\n",
        "                        y_coords[y_coords >= (img_height - crop[1])] = img_height - crop[1] - 1\n",
        "                        targets[:,[2,3]] = y_coords\n",
        "                    if crop[2] > 0:\n",
        "                        x_coords = targets[:,[0,1]]\n",
        "                        x_coords[x_coords < crop[2]] = crop[2]\n",
        "                        targets[:,[0,1]] = x_coords\n",
        "                    if crop[3] > 0:\n",
        "                        x_coords = targets[:,[0,1]]\n",
        "                        x_coords[x_coords >= (img_width - crop[3])] = img_width - crop[3] - 1\n",
        "                        targets[:,[0,1]] = x_coords\n",
        "                   \n",
        "                    before_area = (before_limiting[:,1] - before_limiting[:,0]) * (before_limiting[:,3] - before_limiting[:,2])\n",
        "                    after_area = (targets[:,1] - targets[:,0]) * (targets[:,3] - targets[:,2])\n",
        "                    targets = targets[after_area >= include_thresh * before_area]\n",
        "                \n",
        "                if crop[0] > 0:\n",
        "                    targets[:,[2,3]] -= crop[0]\n",
        "                if crop[2] > 0:\n",
        "                    targets[:,[0,1]] -= crop[2]\n",
        "                img_height -= crop[0] - crop[1]\n",
        "                img_width -= crop[2] - crop[3]\n",
        "\n",
        "            if resize:\n",
        "                image = cv2.resize(image, dsize=resize)\n",
        "                targets[:,[0,1]] = (targets[:,[0,1]] * (resize[0] / img_width)).astype(np.int)\n",
        "                targets[:,[2,3]] = (targets[:,[2,3]] * (resize[1] / img_height)).astype(np.int)\n",
        "\n",
        "            if gray:\n",
        "                image = np.expand_dims(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY), 3)\n",
        "\n",
        "            if diagnostics:\n",
        "                processed_images.append(image)\n",
        "                processed_labels.append(targets)\n",
        "\n",
        "            img = Image.fromarray(image.astype(np.uint8))\n",
        "            img.save('{}{}'.format(dest_path, filename), 'JPEG', quality=90)\n",
        "            del image\n",
        "            del img\n",
        "            gc.collect()\n",
        "\n",
        "            # Transform the labels back to the original CSV file format:\n",
        "            # One line per ground truth box, i.e. possibly multiple lines per image\n",
        "            for target in targets:\n",
        "                target = list(target)\n",
        "                target = [filename] + target\n",
        "                targets_for_csv.append(target)\n",
        "\n",
        "        with open('{}labels.csv'.format(dest_path), 'w', newline='') as csvfile:\n",
        "            labelswriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "            labelswriter.writerow(['frame', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'])\n",
        "            labelswriter.writerows(targets_for_csv)\n",
        "\n",
        "        if diagnostics:\n",
        "            print(\"Image processing completed.\")\n",
        "            return np.array(processed_images), np.array(original_images), np.array(targets_for_csv), processed_labels\n",
        "        else:\n",
        "            print(\"Image processing completed.\")\n",
        " \n",
        "#ssd_batch_generator compelete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEgkVOvUHS2H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c_z1PWROgI3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evJa0HP9gbOt",
        "colab_type": "code",
        "outputId": "b059d304-cfc7-40dc-f15e-c9a95198e0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vdTKrUjvgbMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7j1CBktwHVTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6oFM-JPQNdUo",
        "colab_type": "code",
        "outputId": "f22784e8-adc1-4805-b6ac-c8cce44a8f82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "img_height = 300 \n",
        "img_width = 480 \n",
        "img_channels = 3 \n",
        "subtract_mean = 127.5  \n",
        "divide_by_stddev = 127.5 \n",
        "n_classes=5\n",
        "scales = [0.08, 0.16, 0.32, 0.64, 0.96] \n",
        "aspect_ratios = [0.5, 1.0, 2.0] \n",
        "two_boxes_for_ar1 = True \n",
        "steps = None \n",
        "offsets = None \n",
        "limit_boxes = False \n",
        "variances = [1.0, 1.0, 1.0, 1.0] \n",
        "coords = 'centroids' \n",
        "normalize_coords = False\n",
        "\n",
        "\n",
        "\n",
        "K.clear_session()  #to clear any prevoius models\n",
        "\n",
        "model = build_model(image_size=(img_height, img_width, img_channels),          # to create a model using build_model function with parameters passed\n",
        "                    n_classes=n_classes,\n",
        "                    l2_regularization=0.0,\n",
        "                    scales=scales,\n",
        "                    aspect_ratios_global=aspect_ratios,\n",
        "                    aspect_ratios_per_layer=None,\n",
        "                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                    steps=steps,\n",
        "                    offsets=offsets,\n",
        "                    limit_boxes=limit_boxes,\n",
        "                    variances=variances,\n",
        "                    coords=coords,\n",
        "                    normalize_coords=normalize_coords,\n",
        "                    subtract_mean=subtract_mean,\n",
        "                    divide_by_stddev=divide_by_stddev,\n",
        "                    swap_channels=False)\n",
        "\n",
        "\n",
        "\n",
        "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)      # optimiser values\n",
        "\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)                      #loss function\n",
        "\n",
        "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)                        #create model\n",
        "\n",
        "train_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
        "val_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
        "\n",
        "\n",
        "train_images_dir      = '/gdrive/My Drive/udacity_driving_datasets'\n",
        "train_labels_filename = '/gdrive/My Drive/udacity_driving_datasets/labels_train.csv'\n",
        "\n",
        "\n",
        "val_images_dir      = '/gdrive/My Drive/udacity_driving_datasets'\n",
        "val_labels_filename = '/gdrive/My Drive/udacity_driving_datasets/labels_val.csv'\n",
        "\n",
        "train_dataset.parse_csv(images_dir=train_images_dir,\n",
        "                        labels_filename=train_labels_filename,\n",
        "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], \n",
        "                        include_classes='all')\n",
        "\n",
        "val_dataset.parse_csv(images_dir=val_images_dir,\n",
        "                      labels_filename=val_labels_filename,\n",
        "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
        "                      include_classes='all')\n",
        "\n",
        "predictor_sizes = [model.get_layer('classes4').output_shape[1:3],\n",
        "                   model.get_layer('classes5').output_shape[1:3],\n",
        "                   model.get_layer('classes6').output_shape[1:3],\n",
        "                   model.get_layer('classes7').output_shape[1:3]]\n",
        "\n",
        "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
        "                                img_width=img_width,\n",
        "                                n_classes=n_classes, \n",
        "                                predictor_sizes=predictor_sizes,\n",
        "                                scales=scales,\n",
        "                                aspect_ratios_global=aspect_ratios,\n",
        "                                aspect_ratios_per_layer=None,\n",
        "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                steps=steps,\n",
        "                                offsets=offsets,\n",
        "                                limit_boxes=limit_boxes,\n",
        "                                variances=variances,\n",
        "                                pos_iou_threshold=0.5,\n",
        "                                neg_iou_threshold=0.2,\n",
        "                                coords=coords,\n",
        "                                normalize_coords=normalize_coords)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32 \n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         train=True,\n",
        "                                         ssd_box_encoder=ssd_box_encoder,\n",
        "                                         equalize=False,\n",
        "                                         brightness=(0.5, 2, 0.5), \n",
        "                                         flip=False, \n",
        "                                         translate=((5, 50), (3, 30), 0.5), \n",
        "                                         scale=(0.75, 1.3, 0.5), \n",
        "                                         max_crop_and_resize=False,\n",
        "                                         random_pad_and_resize=False,\n",
        "                                         random_crop=False,\n",
        "                                         crop=False,\n",
        "                                         resize=False,\n",
        "                                         gray=False,\n",
        "                                         limit_boxes=True,\n",
        "                                         include_thresh=0.4)\n",
        "\n",
        "val_generator = val_dataset.generate(batch_size=batch_size,\n",
        "                                     shuffle=True,\n",
        "                                     train=True,\n",
        "                                     ssd_box_encoder=ssd_box_encoder,\n",
        "                                     equalize=False,\n",
        "                                     brightness=False,\n",
        "                                     flip=False,\n",
        "                                     translate=False,\n",
        "                                     scale=False,\n",
        "                                     max_crop_and_resize=False,\n",
        "                                     random_pad_and_resize=False,\n",
        "                                     random_crop=False,\n",
        "                                     crop=False,\n",
        "                                     resize=False,\n",
        "                                     gray=False,\n",
        "                                     limit_boxes=True,\n",
        "                                     include_thresh=0.4)\n",
        "\n",
        "\n",
        "n_train_samples = train_dataset.get_n_samples()\n",
        "n_val_samples = val_dataset.get_n_samples()\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit_generator(generator = train_generator,\n",
        "                              steps_per_epoch = ceil(n_train_samples/batch_size),\n",
        "                              epochs = epochs,\n",
        "                              callbacks = [ModelCheckpoint('ssd7_weights_epoch-{epoch:02d}_loss-{loss:.4f}.h5',\n",
        "                                                           monitor='val_loss',\n",
        "                                                           verbose=1,\n",
        "                                                           save_best_only=True,\n",
        "                                                           save_weights_only=True,\n",
        "                                                           mode='auto',\n",
        "                                                           period=1),\n",
        "                                           EarlyStopping(monitor='val_loss',\n",
        "                                                         min_delta=0.0001,\n",
        "                                                         patience=10),\n",
        "                                           ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                             factor=0.5,\n",
        "                                                             patience=0,\n",
        "                                                             epsilon=0.001,\n",
        "                                                             cooldown=0)],\n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = ceil(n_val_samples/batch_size))\n",
        "\n",
        "# TODO: Set the filename (without the .h5 file extension!) under which to save the model and weights.\n",
        "#       Do the same in the `ModelCheckpoint` callback above.\n",
        "model_name = 'ssd7'\n",
        "model.save('{}.h5'.format(model_name))\n",
        "model.save_weights('{}_weights.h5'.format(model_name))\n",
        "\n",
        "print()\n",
        "print(\"Model saved under {}.h5\".format(model_name))\n",
        "print(\"Weights also saved separately under {}_weights.h5\".format(model_name))\n",
        "print()\n",
        "\n",
        "\n",
        "predict_generator = val_dataset.generate(batch_size=1,\n",
        "                                         shuffle=True,\n",
        "                                         train=False,\n",
        "                                         returns={'processed_labels',\n",
        "                                                  'filenames'},\n",
        "                                         equalize=False,\n",
        "                                         brightness=False,\n",
        "                                         flip=False,\n",
        "                                         translate=False,\n",
        "                                         scale=False,\n",
        "                                         max_crop_and_resize=False,\n",
        "                                         random_pad_and_resize=False,\n",
        "                                         random_crop=False,\n",
        "                                         crop=False,\n",
        "                                         resize=False,\n",
        "                                         gray=False,\n",
        "                                         limit_boxes=True,\n",
        "                                         include_thresh=0.4)\n",
        "\n",
        "X, y_true, filenames = next(predict_generator)\n",
        "\n",
        "i = 0 # Which batch item to look at\n",
        "\n",
        "print(\"Image:\", filenames[i])\n",
        "print()\n",
        "print(\"Ground truth boxes:\\n\")\n",
        "print(y_true[i])\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "y_pred_decoded = decode_y2(y_pred,\n",
        "                           confidence_thresh=0.5,\n",
        "                           iou_threshold=0.4,\n",
        "                           top_k='all',\n",
        "                           input_coords='centroids',\n",
        "                           normalize_coords=False,\n",
        "                           img_height=None,\n",
        "                           img_width=None)\n",
        "\n",
        "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
        "print(\"Decoded predictions (output format is [class_id, confidence, xmin, ymin, xmax, ymax]):\\n\")\n",
        "print(y_pred_decoded[i])\n",
        "\n",
        "\n",
        "# 5: Draw the predicted boxes onto the image\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(X[i])\n",
        "\n",
        "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
        "current_axis = plt.gca()\n",
        "\n",
        "classes = ['background', 'car', 'truck', 'pedestrian', 'bicyclist', 'light'] # Just so we can print class names onto the image instead of IDs\n",
        "\n",
        "# Draw the ground truth boxes in green (omit the label for more clarity)\n",
        "for box in y_true[i]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
        "    #current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})\n",
        "\n",
        "# Draw the predicted boxes in blue\n",
        "for box in y_pred_decoded[i]:\n",
        "    xmin = box[-4]\n",
        "    ymin = box[-3]\n",
        "    xmax = box[-2]\n",
        "    ymax = box[-1]\n",
        "    color = colors[int(box[0])]\n",
        "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='blue', fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'blue', 'alpha':1.0})\n",
        "\n",
        "print(\"its ok\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " 38/563 [=>............................] - ETA: 2:33:15 - loss: 10.0496"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19o35uhe_ITj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training started at 5:08 pm"
      ]
    }
  ]
}